{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/omarelhakim0/eeg-emotion-classification-on-seediv?scriptVersionId=282223356\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"2842d78f","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2025-11-27T14:40:31.594123Z","iopub.status.busy":"2025-11-27T14:40:31.593864Z","iopub.status.idle":"2025-11-27T14:54:43.786807Z","shell.execute_reply":"2025-11-27T14:54:43.785969Z"},"papermill":{"duration":852.199048,"end_time":"2025-11-27T14:54:43.788369","exception":false,"start_time":"2025-11-27T14:40:31.589321","status":"completed"},"scrolled":true,"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.4/251.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m231.5/231.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.1/295.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m750.2/750.2 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n","  Building wheel for torcheeg (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n","  Building wheel for spectrum (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","kaggle-environments 1.18.0 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\r\n","cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n","jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\r\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\r\n","dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n","scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\r\n","libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n","cvxpy 1.6.7 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\r\n","imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\r\n","xarray-einstats 0.9.1 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\r\n","plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n","pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n","pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n","jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\r\n","umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\r\n","mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0m"]}],"source":["!pip install torch_scatter torcheeg -qq"]},{"cell_type":"code","execution_count":2,"id":"40651cda","metadata":{"execution":{"iopub.execute_input":"2025-11-27T14:54:43.832031Z","iopub.status.busy":"2025-11-27T14:54:43.831751Z","iopub.status.idle":"2025-11-27T14:54:43.835312Z","shell.execute_reply":"2025-11-27T14:54:43.834771Z"},"papermill":{"duration":0.026686,"end_time":"2025-11-27T14:54:43.836444","exception":false,"start_time":"2025-11-27T14:54:43.809758","status":"completed"},"tags":[]},"outputs":[],"source":["# !rm -rf tmp_out"]},{"cell_type":"code","execution_count":3,"id":"20612da8","metadata":{"execution":{"iopub.execute_input":"2025-11-27T14:54:43.878582Z","iopub.status.busy":"2025-11-27T14:54:43.878348Z","iopub.status.idle":"2025-11-27T14:54:55.683962Z","shell.execute_reply":"2025-11-27T14:54:55.683132Z"},"papermill":{"duration":11.828584,"end_time":"2025-11-27T14:54:55.685467","exception":false,"start_time":"2025-11-27T14:54:43.856883","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.utils as utils\n","from torch.utils.data import DataLoader, Subset\n","from torcheeg.models import CCNN\n","from torcheeg import transforms\n","from torcheeg.transforms import ToGrid\n","from torcheeg.datasets import SEEDIVDataset,SEEDIVFeatureDataset\n","from torcheeg.datasets.constants import SEED_IV_CHANNEL_LOCATION_DICT\n","\n","import scipy.signal as signal\n","import random"]},{"cell_type":"code","execution_count":4,"id":"e593c1fa","metadata":{"execution":{"iopub.execute_input":"2025-11-27T14:54:55.729503Z","iopub.status.busy":"2025-11-27T14:54:55.728549Z","iopub.status.idle":"2025-11-27T14:54:55.759149Z","shell.execute_reply":"2025-11-27T14:54:55.758245Z"},"papermill":{"duration":0.054231,"end_time":"2025-11-27T14:54:55.760655","exception":false,"start_time":"2025-11-27T14:54:55.706424","status":"completed"},"tags":[]},"outputs":[],"source":["# 1. Setup Device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":5,"id":"ca398ca6","metadata":{"execution":{"iopub.execute_input":"2025-11-27T14:54:55.809031Z","iopub.status.busy":"2025-11-27T14:54:55.808436Z","iopub.status.idle":"2025-11-27T14:54:55.812458Z","shell.execute_reply":"2025-11-27T14:54:55.81183Z"},"papermill":{"duration":0.028809,"end_time":"2025-11-27T14:54:55.813639","exception":false,"start_time":"2025-11-27T14:54:55.78483","status":"completed"},"tags":[]},"outputs":[],"source":["def BandPassFilter(eeg_data):\n","    b, a = signal.butter(4, Wn=[1.0, 75.0], btype='bandpass', fs=200)\n","    return signal.filtfilt(b, a, eeg_data, axis=-1)"]},{"cell_type":"code","execution_count":6,"id":"6629b96d","metadata":{"execution":{"iopub.execute_input":"2025-11-27T14:54:55.859437Z","iopub.status.busy":"2025-11-27T14:54:55.858928Z","iopub.status.idle":"2025-11-27T14:54:55.862819Z","shell.execute_reply":"2025-11-27T14:54:55.862279Z"},"papermill":{"duration":0.028383,"end_time":"2025-11-27T14:54:55.863969","exception":false,"start_time":"2025-11-27T14:54:55.835586","status":"completed"},"tags":[]},"outputs":[],"source":["def Notch(eeg_data):\n","    b, a = signal.iirnotch(w0=50.0, Q=30.0, fs=200)\n","    return signal.filtfilt(b, a, eeg_data, axis=-1)"]},{"cell_type":"code","execution_count":7,"id":"21f99a42","metadata":{"execution":{"iopub.execute_input":"2025-11-27T14:54:55.905526Z","iopub.status.busy":"2025-11-27T14:54:55.905124Z","iopub.status.idle":"2025-11-27T14:54:55.908593Z","shell.execute_reply":"2025-11-27T14:54:55.908051Z"},"papermill":{"duration":0.025311,"end_time":"2025-11-27T14:54:55.909685","exception":false,"start_time":"2025-11-27T14:54:55.884374","status":"completed"},"tags":[]},"outputs":[],"source":["# 2. Define Preprocessing\n","t_transform = transforms.Compose([\n","    transforms.Lambda(BandPassFilter),\n","    transforms.Lambda(Notch),\n","    transforms.BaselineRemoval(),\n","    transforms.MeanStdNormalize(),\n","    transforms.To2d()\n","    \n","])"]},{"cell_type":"markdown","id":"2f822e9e","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2025-11-27T13:33:37.662075Z","iopub.status.busy":"2025-11-27T13:33:37.661785Z","iopub.status.idle":"2025-11-27T13:33:38.016401Z","shell.execute_reply":"2025-11-27T13:33:38.015694Z","shell.execute_reply.started":"2025-11-27T13:33:37.662055Z"},"papermill":{"duration":0.020396,"end_time":"2025-11-27T14:54:55.950086","exception":false,"start_time":"2025-11-27T14:54:55.92969","status":"completed"},"tags":[]},"source":["# 3. Load Data\n","dataset = SEEDIVDataset(\n","    io_path='./tmp_out/seed_iv',\n","    root_path='/kaggle/input/seed-iv/eeg_raw_data',\n","    offline_transform=t_transform,\n","    label_transform=transforms.Compose([\n","        transforms.Select('emotion'),\n","    ]),\n","    chunk_size=800,  # 4 seconds\n","    num_worker=4\n",")"]},{"cell_type":"code","execution_count":8,"id":"5095a694","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2025-11-27T14:54:55.992376Z","iopub.status.busy":"2025-11-27T14:54:55.992096Z","iopub.status.idle":"2025-11-27T14:56:32.11985Z","shell.execute_reply":"2025-11-27T14:56:32.119207Z"},"papermill":{"duration":96.150755,"end_time":"2025-11-27T14:56:32.1212","exception":false,"start_time":"2025-11-27T14:54:55.970445","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[2025-11-27 14:54:55] INFO (torcheeg/MainThread) ðŸ” | Processing EEG data. Processed EEG data has been cached to \u001b[92m./tmp_out/seed_iv_features\u001b[0m.\n","[2025-11-27 14:54:55] INFO (torcheeg/MainThread) â³ | Monitoring the detailed processing of a record for debugging. The processing of other records will only be reported in percentage to keep it clean.\n","\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 0it [00:00, ?it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 1it [00:00,  4.50it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 14it [00:00, 52.12it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 24it [00:00, 68.05it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 34it [00:00, 78.08it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 45it [00:00, 87.27it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 56it [00:00, 92.66it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 68it [00:00, 99.20it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 80it [00:00, 103.74it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 91it [00:01, 103.97it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 102it [00:01, 104.98it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 113it [00:01, 104.85it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 124it [00:01, 105.62it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 136it [00:01, 109.18it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 147it [00:01, 106.92it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 158it [00:01, 106.41it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 169it [00:01, 105.27it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 180it [00:01, 99.23it/s] \u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 191it [00:02, 99.36it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 203it [00:02, 102.36it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 214it [00:02, 104.37it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 225it [00:02, 103.49it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 236it [00:02, 103.95it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 248it [00:02, 105.98it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 260it [00:02, 109.54it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 271it [00:02, 104.48it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 282it [00:02, 103.66it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 294it [00:02, 106.58it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 305it [00:03, 107.47it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 317it [00:03, 108.93it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 329it [00:03, 110.54it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 341it [00:03, 110.44it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 353it [00:03, 111.11it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 365it [00:03, 111.86it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 377it [00:03, 111.59it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 389it [00:03, 111.89it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 401it [00:03, 109.95it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 413it [00:04, 108.36it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 424it [00:04, 107.99it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 435it [00:04, 108.56it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 447it [00:04, 110.24it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 459it [00:04, 109.23it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 470it [00:04, 102.70it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 481it [00:04, 102.58it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 492it [00:04, 103.29it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 503it [00:04, 103.35it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 514it [00:05, 104.90it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 525it [00:05, 105.55it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 536it [00:05, 106.65it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 548it [00:05, 108.34it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 560it [00:05, 110.87it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 572it [00:05, 110.49it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 584it [00:05, 111.08it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 596it [00:05, 111.02it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 608it [00:05, 111.01it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 620it [00:05, 111.51it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 632it [00:06, 111.93it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 644it [00:06, 112.15it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 656it [00:06, 112.30it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 668it [00:06, 111.48it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 680it [00:06, 110.08it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 692it [00:07, 51.60it/s] \u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 703it [00:07, 60.49it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 715it [00:07, 70.79it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 726it [00:07, 77.59it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 736it [00:07, 82.57it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 747it [00:07, 87.92it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 758it [00:07, 93.48it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 770it [00:07, 97.86it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 781it [00:07, 99.41it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 792it [00:07, 101.37it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 803it [00:08, 102.00it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 814it [00:08, 104.08it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 826it [00:08, 106.85it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 837it [00:08, 105.15it/s]\u001b[A\n","[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 848it [00:08, 103.93it/s]\u001b[A\n","                                                                                             \u001b[A[2025-11-27 14:56:32] INFO (torcheeg/MainThread) âœ… | All processed EEG data has been cached to ./tmp_out/seed_iv_features.\n","[2025-11-27 14:56:32] INFO (torcheeg/MainThread) ðŸ˜Š | Please set \u001b[92mio_path\u001b[0m to \u001b[92m./tmp_out/seed_iv_features\u001b[0m for the next run, to directly read from the cache if you wish to skip the data processing step.\n"]}],"source":["%%capture\n","# Define the new pipeline for FEATURES\n","dataset = SEEDIVFeatureDataset(\n","    io_path='./tmp_out/seed_iv_features',\n","    root_path='/kaggle/input/seed-iv/eeg_feature_smooth',\n","    feature=['de_LDS'], # Using Differential Entropy (Best for emotion)\n","    num_worker=4,        # Keep 0 for Kaggle stability\n","    offline_transform=transforms.Compose([\n","        # CONVERT 62 VECTORS -> 9x9 IMAGE\n","        ToGrid(SEED_IV_CHANNEL_LOCATION_DICT),\n","        transforms.Lambda(lambda x: torch.tensor(x).float())\n","    ]),\n","    label_transform=transforms.Select('emotion')\n",")\n","\n","print(f\"Loaded Features. Shape per sample: {dataset[0][0].shape}\")\n","# Expected Output: torch.Size([5, 9, 9]) \n","# (5 Bands: Delta/Theta/Alpha/Beta/Gamma, 9x9 Grid)"]},{"cell_type":"code","execution_count":9,"id":"69a09c9a","metadata":{"execution":{"iopub.execute_input":"2025-11-27T14:56:32.169864Z","iopub.status.busy":"2025-11-27T14:56:32.169171Z","iopub.status.idle":"2025-11-27T14:56:32.184975Z","shell.execute_reply":"2025-11-27T14:56:32.184078Z"},"papermill":{"duration":0.041278,"end_time":"2025-11-27T14:56:32.186228","exception":false,"start_time":"2025-11-27T14:56:32.14495","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Segments: 37575\n","------------------------------\n","Count per Emotion:\n","emotion\n","0    10170\n","1    10245\n","2     9225\n","3     7935\n","Name: count, dtype: int64\n","------------------------------\n","Percentage per Emotion:\n","emotion\n","0    27.07\n","1    27.27\n","2    24.55\n","3    21.12\n","Name: count, dtype: float64\n","\n","âœ… Data is reasonably BALANCED (Diff: 6.15%)\n"]}],"source":["import pandas as pd\n","\n","# 1. Get the metadata DataFrame\n","df = dataset.info\n","\n","# 2. Count the segments for each emotion\n","# 0: Neutral, 1: Sad, 2: Fear, 3: Happy\n","counts = df['emotion'].value_counts().sort_index()\n","total = len(df)\n","\n","print(f\"Total Segments: {total}\")\n","print(\"-\" * 30)\n","print(\"Count per Emotion:\")\n","print(counts)\n","\n","print(\"-\" * 30)\n","print(\"Percentage per Emotion:\")\n","percentages = (counts / total) * 100\n","print(percentages.round(2))\n","\n","# 3. Check for Imbalance\n","# If the difference between max and min is > 10%, we might need a WeightedSampler\n","max_pct = percentages.max()\n","min_pct = percentages.min()\n","\n","if (max_pct - min_pct) > 10:\n","    print(f\"\\nâš ï¸ WARNING: Data is IMBALANCED (Diff: {max_pct - min_pct:.2f}%)\")\n","    print(\"Consider using a WeightedRandomSampler.\")\n","else:\n","    print(f\"\\nâœ… Data is reasonably BALANCED (Diff: {max_pct - min_pct:.2f}%)\")"]},{"cell_type":"code","execution_count":10,"id":"8156c3d3","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2025-11-27T14:56:32.23747Z","iopub.status.busy":"2025-11-27T14:56:32.236819Z","iopub.status.idle":"2025-11-27T14:56:32.248803Z","shell.execute_reply":"2025-11-27T14:56:32.247918Z"},"papermill":{"duration":0.037878,"end_time":"2025-11-27T14:56:32.250161","exception":false,"start_time":"2025-11-27T14:56:32.212283","status":"completed"},"tags":[]},"outputs":[],"source":["# Split by Trial ID\n","# SEED-IV has 24 trials (videos) per session.\n","# 80% of VIDEOS for training (19 videos), 20% for testing (5 videos).\n","all_trial_ids = list(range(1, 25))\n","\n","random.seed(42)\n","test_trial_ids = random.sample(all_trial_ids, 5)\n","train_trial_ids = [t for t in all_trial_ids if t not in test_trial_ids]\n","\n","train_indices = df[df['trial_id'].isin(train_trial_ids)].index.tolist()\n","test_indices = df[df['trial_id'].isin(test_trial_ids)].index.tolist()\n","\n","# Create Subsets & Loaders\n","train_set = Subset(dataset, train_indices)\n","test_set = Subset(dataset, test_indices)\n","\n","train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"]},{"cell_type":"code","execution_count":11,"id":"10d432f9","metadata":{"execution":{"iopub.execute_input":"2025-11-27T14:56:32.298294Z","iopub.status.busy":"2025-11-27T14:56:32.297822Z","iopub.status.idle":"2025-11-27T14:56:32.516812Z","shell.execute_reply":"2025-11-27T14:56:32.516216Z"},"papermill":{"duration":0.244142,"end_time":"2025-11-27T14:56:32.518111","exception":false,"start_time":"2025-11-27T14:56:32.273969","status":"completed"},"tags":[]},"outputs":[],"source":["model = CCNN(\n","    num_classes=4,      # Neutral, Sad, Fear, Happy\n","    in_channels=5,      # 5 Frequency Bands (Delta -> Gamma)\n","    grid_size=(9, 9),   # Matching the ToGrid output\n","    dropout=0.5\n",").to(device)"]},{"cell_type":"code","execution_count":12,"id":"e2af4227","metadata":{"execution":{"iopub.execute_input":"2025-11-27T14:56:32.567857Z","iopub.status.busy":"2025-11-27T14:56:32.567253Z","iopub.status.idle":"2025-11-27T14:56:32.571713Z","shell.execute_reply":"2025-11-27T14:56:32.571087Z"},"papermill":{"duration":0.030352,"end_time":"2025-11-27T14:56:32.572802","exception":false,"start_time":"2025-11-27T14:56:32.54245","status":"completed"},"tags":[]},"outputs":[],"source":["optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n","# Scheduler: If validation loss doesn't go down for 3 epochs, cut LR by half\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":13,"id":"cc7199be","metadata":{"execution":{"iopub.execute_input":"2025-11-27T14:56:32.62215Z","iopub.status.busy":"2025-11-27T14:56:32.621914Z","iopub.status.idle":"2025-11-27T15:00:47.04662Z","shell.execute_reply":"2025-11-27T15:00:47.045799Z"},"papermill":{"duration":254.451073,"end_time":"2025-11-27T15:00:47.047821","exception":false,"start_time":"2025-11-27T14:56:32.596748","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:1538: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n","  warnings.warn(warn_msg)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss=1.3873 (26.1%) | Val Loss=1.3885 (17.3%)\n","  --> Saved Best Model (Acc: 17.28%)\n","Epoch 2: Train Loss=1.3852 (26.4%) | Val Loss=1.3865 (17.3%)\n","  --> Saved Best Model (Acc: 17.28%)\n","Epoch 3: Train Loss=1.3849 (26.3%) | Val Loss=1.3869 (17.3%)\n","Epoch 4: Train Loss=1.3793 (27.8%) | Val Loss=1.3581 (36.2%)\n","  --> Saved Best Model (Acc: 36.24%)\n","Epoch 5: Train Loss=1.3086 (35.8%) | Val Loss=1.4419 (34.3%)\n","Epoch 6: Train Loss=1.1276 (50.8%) | Val Loss=1.4277 (37.8%)\n","Epoch 7: Train Loss=0.9993 (58.6%) | Val Loss=1.2209 (46.5%)\n","  --> Saved Best Model (Acc: 46.51%)\n","Epoch 8: Train Loss=0.8916 (64.2%) | Val Loss=1.2557 (48.1%)\n","Epoch 9: Train Loss=0.7881 (69.5%) | Val Loss=1.2025 (55.6%)\n","  --> Saved Best Model (Acc: 55.65%)\n","Epoch 10: Train Loss=0.7190 (72.5%) | Val Loss=0.9988 (60.3%)\n","  --> Saved Best Model (Acc: 60.28%)\n","Epoch 11: Train Loss=0.6659 (74.9%) | Val Loss=1.1500 (54.3%)\n","Epoch 12: Train Loss=0.5931 (78.4%) | Val Loss=1.2101 (53.5%)\n","Epoch 13: Train Loss=0.5598 (79.7%) | Val Loss=1.0636 (62.6%)\n","Epoch 14: Train Loss=0.5122 (81.5%) | Val Loss=1.0959 (57.9%)\n","Epoch 15: Train Loss=0.4894 (82.4%) | Val Loss=1.4174 (56.4%)\n","Epoch 16: Train Loss=0.4175 (85.6%) | Val Loss=1.4692 (53.7%)\n","Epoch 17: Train Loss=0.3217 (89.8%) | Val Loss=1.2673 (58.3%)\n","Epoch 18: Train Loss=0.2787 (91.3%) | Val Loss=1.2634 (59.9%)\n","Epoch 19: Train Loss=0.2642 (91.6%) | Val Loss=1.3081 (57.3%)\n","Epoch 20: Train Loss=0.2352 (92.9%) | Val Loss=1.3946 (58.7%)\n","Early Stopping.\n"]}],"source":["best_val_loss = float('inf')\n","patience_limit = 10  # Increased patience because we are learning slower\n","counter = 0\n","\n","for epoch in range(50): # Run longer because LR is smaller\n","    # --- TRAINING ---\n","    model.train()\n","    train_loss = 0\n","    correct_train = 0\n","    total_train = 0\n","    \n","    for batch in train_loader:\n","        X = batch[0].to(device).float()\n","        y = batch[1].to(device).long()\n","        \n","        optimizer.zero_grad()\n","        outputs = model(X)\n","        loss = criterion(outputs, y)\n","        loss.backward()\n","        \n","        # --- CHANGE 2: Gradient Clipping ---\n","        # This prevents the \"Loss = 39.0\" explosions\n","        utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        \n","        optimizer.step()\n","        \n","        train_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_train += y.size(0)\n","        correct_train += (predicted == y).sum().item()\n","        \n","    avg_train_loss = train_loss / len(train_loader)\n","    train_acc = (correct_train / total_train) * 100\n","\n","    # --- VALIDATION ---\n","    model.eval()\n","    val_loss = 0\n","    correct_val = 0\n","    total_val = 0\n","    \n","    with torch.no_grad():\n","        for batch in test_loader:\n","            X = batch[0].to(device).float()\n","            y = batch[1].to(device).long()\n","            \n","            outputs = model(X)\n","            loss = criterion(outputs, y)\n","            \n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_val += y.size(0)\n","            correct_val += (predicted == y).sum().item()\n","            \n","    avg_val_loss = val_loss / len(test_loader)\n","    val_acc = (correct_val / total_val) * 100\n","    \n","    # Update Scheduler\n","    scheduler.step(avg_val_loss)\n","\n","    print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f} ({train_acc:.1f}%) | Val Loss={avg_val_loss:.4f} ({val_acc:.1f}%)\")\n","\n","    # --- SAVE BEST ---\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        torch.save(model.state_dict(), 'best_eegnet_stable.pth')\n","        print(f\"  --> Saved Best Model (Acc: {val_acc:.2f}%)\")\n","        counter = 0\n","    else:\n","        counter += 1\n","        if counter >= patience_limit:\n","            print(\"Early Stopping.\")\n","            break"]},{"cell_type":"code","execution_count":null,"id":"c79df507","metadata":{"papermill":{"duration":0.024127,"end_time":"2025-11-27T15:00:47.096508","exception":false,"start_time":"2025-11-27T15:00:47.072381","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":218098,"sourceId":472319,"sourceType":"datasetVersion"}],"dockerImageVersionId":31192,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":1222.294494,"end_time":"2025-11-27T15:00:49.837922","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-27T14:40:27.543428","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}