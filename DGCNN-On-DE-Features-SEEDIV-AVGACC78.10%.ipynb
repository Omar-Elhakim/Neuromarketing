{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84131981",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-11T10:29:46.151332Z",
     "iopub.status.busy": "2025-12-11T10:29:46.151058Z",
     "iopub.status.idle": "2025-12-11T10:44:33.013756Z",
     "shell.execute_reply": "2025-12-11T10:44:33.012912Z"
    },
    "papermill": {
     "duration": 886.867931,
     "end_time": "2025-12-11T10:44:33.015489",
     "exception": false,
     "start_time": "2025-12-11T10:29:46.147558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m251.4/251.4 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m231.5/231.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m295.1/295.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m750.2/750.2 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Building wheel for torcheeg (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Building wheel for spectrum (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "kaggle-environments 1.18.0 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\r\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "cvxpy 1.6.7 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\r\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "xarray-einstats 0.9.1 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\r\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch_scatter torcheeg torch_geometric -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ff029e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T10:44:33.063034Z",
     "iopub.status.busy": "2025-12-11T10:44:33.062718Z",
     "iopub.status.idle": "2025-12-11T10:44:49.399096Z",
     "shell.execute_reply": "2025-12-11T10:44:49.398464Z"
    },
    "papermill": {
     "duration": 16.362331,
     "end_time": "2025-12-11T10:44:49.400455",
     "exception": false,
     "start_time": "2025-12-11T10:44:33.038124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as utils\n",
    "from torch.utils.data import DataLoader, Subset ,WeightedRandomSampler\n",
    "from torcheeg.models import CCNN\n",
    "from torcheeg import transforms\n",
    "from torcheeg.transforms import ToGrid\n",
    "from torcheeg.datasets import SEEDIVDataset,SEEDIVFeatureDataset\n",
    "from torcheeg.datasets.constants import SEED_IV_CHANNEL_LOCATION_DICT\n",
    "from torcheeg.transforms import ToG\n",
    "from torcheeg.datasets.constants import SEED_IV_ADJACENCY_MATRIX\n",
    "from torcheeg.models import DGCNN\n",
    "import torch_geometric.loader as geom_loader # Special loader for graphs\n",
    "import copy\n",
    "import scipy.signal as signal\n",
    "import random\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc1b93fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T10:44:49.444186Z",
     "iopub.status.busy": "2025-12-11T10:44:49.443348Z",
     "iopub.status.idle": "2025-12-11T10:44:49.448057Z",
     "shell.execute_reply": "2025-12-11T10:44:49.447304Z"
    },
    "papermill": {
     "duration": 0.027615,
     "end_time": "2025-12-11T10:44:49.449090",
     "exception": false,
     "start_time": "2025-12-11T10:44:49.421475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f92121b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T10:44:49.493386Z",
     "iopub.status.busy": "2025-12-11T10:44:49.492640Z",
     "iopub.status.idle": "2025-12-11T10:44:49.505795Z",
     "shell.execute_reply": "2025-12-11T10:44:49.505244Z"
    },
    "papermill": {
     "duration": 0.036563,
     "end_time": "2025-12-11T10:44:49.506981",
     "exception": false,
     "start_time": "2025-12-11T10:44:49.470418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across different libraries.\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8ee29b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T10:44:49.550404Z",
     "iopub.status.busy": "2025-12-11T10:44:49.549660Z",
     "iopub.status.idle": "2025-12-11T10:48:08.146386Z",
     "shell.execute_reply": "2025-12-11T10:48:08.145616Z"
    },
    "papermill": {
     "duration": 198.619585,
     "end_time": "2025-12-11T10:48:08.147708",
     "exception": false,
     "start_time": "2025-12-11T10:44:49.528123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-11 10:44:49] INFO (torcheeg/MainThread) üîç | Processing EEG data. Processed EEG data has been cached to \u001b[92m./tmp_out/seed_iv_features\u001b[0m.\n",
      "[2025-12-11 10:44:49] INFO (torcheeg/MainThread) ‚è≥ | Monitoring the detailed processing of a record for debugging. The processing of other records will only be reported in percentage to keep it clean.\n",
      "[PROCESS]:   0%|          | 0/45 [00:00<?, ?it/s]\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 0it [00:00, ?it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 1it [00:00,  4.73it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 17it [00:00, 65.72it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 35it [00:00, 106.55it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 54it [00:00, 132.54it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 73it [00:00, 150.85it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 94it [00:00, 167.89it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 114it [00:00, 175.95it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 135it [00:00, 184.12it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 154it [00:01, 183.53it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 173it [00:01, 183.85it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 192it [00:01, 183.88it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 211it [00:01, 183.94it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 230it [00:01, 185.26it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 251it [00:01, 190.47it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 272it [00:01, 194.44it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 292it [00:01, 194.32it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 312it [00:01, 195.34it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 332it [00:01, 194.96it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 353it [00:02, 198.71it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 373it [00:02, 196.16it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 393it [00:02, 195.23it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 413it [00:02, 191.71it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 433it [00:02, 193.64it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 453it [00:02, 193.24it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 473it [00:02, 190.61it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 493it [00:02, 187.37it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 512it [00:02, 186.28it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 531it [00:03, 184.34it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 550it [00:03, 184.41it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 569it [00:03, 183.68it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 591it [00:03, 191.76it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 613it [00:03, 198.51it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 634it [00:03, 200.70it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 655it [00:03, 195.78it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 675it [00:03, 191.45it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 695it [00:03, 183.17it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 714it [00:03, 180.56it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 734it [00:04, 184.33it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 754it [00:04, 187.94it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 774it [00:04, 189.20it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 793it [00:04, 186.79it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 812it [00:04, 174.34it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 830it [00:04, 174.86it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 850it [00:04, 179.01it/s]\u001b[A\n",
      "[PROCESS]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [03:18<00:00,  4.41s/it]\n",
      "[2025-12-11 10:48:08] INFO (torcheeg/MainThread) ‚úÖ | All processed EEG data has been cached to ./tmp_out/seed_iv_features.\n",
      "[2025-12-11 10:48:08] INFO (torcheeg/MainThread) üòä | Please set \u001b[92mio_path\u001b[0m to \u001b[92m./tmp_out/seed_iv_features\u001b[0m for the next run, to directly read from the cache if you wish to skip the data processing step.\n"
     ]
    }
   ],
   "source": [
    "dataset = SEEDIVFeatureDataset(\n",
    "    io_path='./tmp_out/seed_iv_features',\n",
    "    root_path='/kaggle/input/seed-iv/eeg_feature_smooth',\n",
    "    feature=['de_LDS'], \n",
    "    num_worker=0,\n",
    "    offline_transform=transforms.Compose([\n",
    "        transforms.To2d(), \n",
    "        transforms.Lambda(lambda x: torch.tensor(x).float())]),\n",
    "    label_transform=transforms.Select('emotion'),\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f7d35b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T10:48:08.205462Z",
     "iopub.status.busy": "2025-12-11T10:48:08.205185Z",
     "iopub.status.idle": "2025-12-11T10:48:08.214049Z",
     "shell.execute_reply": "2025-12-11T10:48:08.213206Z"
    },
    "papermill": {
     "duration": 0.038831,
     "end_time": "2025-12-11T10:48:08.215129",
     "exception": false,
     "start_time": "2025-12-11T10:48:08.176298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Labels in Data: [0, 1, 2, 3]\n",
      "Subjects Found: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "# This DataFrame contains columns like: subject_id, trial_id, emotion, etc.\n",
    "meta_info = dataset.info \n",
    "all_subjects = sorted(meta_info['subject_id'].unique())\n",
    "original_labels = meta_info['emotion'].unique()\n",
    "\n",
    "print(f\"Original Labels in Data: {sorted(original_labels)}\")\n",
    "print(f\"Subjects Found: {all_subjects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63569dcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T10:48:08.272691Z",
     "iopub.status.busy": "2025-12-11T10:48:08.272030Z",
     "iopub.status.idle": "2025-12-11T10:48:08.276162Z",
     "shell.execute_reply": "2025-12-11T10:48:08.275404Z"
    },
    "papermill": {
     "duration": 0.034361,
     "end_time": "2025-12-11T10:48:08.277305",
     "exception": false,
     "start_time": "2025-12-11T10:48:08.242944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.005\n",
    "PATIENCE_LR = 3\n",
    "REDUCE_FACTOR = 0.5\n",
    "PATIENCE_ES = 15\n",
    "WEIGHT_DECAY = 0.01\n",
    "HIDE_CHANNELS = 32\n",
    "LABEL_SMOOTHING = 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8014bd1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T10:48:08.336374Z",
     "iopub.status.busy": "2025-12-11T10:48:08.336041Z",
     "iopub.status.idle": "2025-12-11T10:53:49.215170Z",
     "shell.execute_reply": "2025-12-11T10:53:49.214060Z"
    },
    "papermill": {
     "duration": 340.909895,
     "end_time": "2025-12-11T10:53:49.216555",
     "exception": false,
     "start_time": "2025-12-11T10:48:08.306660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Subject-Dependent Evaluation...\n",
      "\n",
      "============================== Processing Subject 1 ==============================\n",
      "Epoch 01 | Train Loss: 1.1265 Acc: 70.07% | Val Loss: 1.2126 Acc: 56.80%\n",
      "Epoch 02 | Train Loss: 0.8143 Acc: 95.56% | Val Loss: 1.2273 Acc: 71.14%\n",
      "Epoch 03 | Train Loss: 0.7832 Acc: 97.71% | Val Loss: 1.1441 Acc: 69.49%\n",
      "Epoch 04 | Train Loss: 0.7592 Acc: 98.88% | Val Loss: 1.4175 Acc: 48.16%\n",
      "Epoch 05 | Train Loss: 0.7510 Acc: 99.24% | Val Loss: 1.1350 Acc: 80.70%\n",
      "Epoch 06 | Train Loss: 0.7658 Acc: 97.71% | Val Loss: 1.2410 Acc: 55.15%\n",
      "Epoch 07 | Train Loss: 0.7386 Acc: 99.54% | Val Loss: 1.2569 Acc: 61.95%\n",
      "Epoch 08 | Train Loss: 0.7379 Acc: 99.59% | Val Loss: 1.3077 Acc: 51.47%\n",
      "Epoch 09 | Train Loss: 0.7361 Acc: 99.75% | Val Loss: 1.2489 Acc: 52.21%\n",
      "Epoch 10 | Train Loss: 0.7201 Acc: 99.90% | Val Loss: 1.2715 Acc: 59.93%\n",
      "Epoch 11 | Train Loss: 0.7189 Acc: 100.00% | Val Loss: 1.3256 Acc: 55.15%\n",
      "Epoch 12 | Train Loss: 0.7191 Acc: 100.00% | Val Loss: 1.1983 Acc: 61.21%\n",
      "Epoch 13 | Train Loss: 0.7160 Acc: 100.00% | Val Loss: 1.2923 Acc: 53.12%\n",
      "Epoch 14 | Train Loss: 0.7115 Acc: 100.00% | Val Loss: 1.2764 Acc: 55.15%\n",
      "Epoch 15 | Train Loss: 0.7096 Acc: 100.00% | Val Loss: 1.2738 Acc: 52.76%\n",
      "Epoch 16 | Train Loss: 0.7071 Acc: 100.00% | Val Loss: 1.2141 Acc: 54.60%\n",
      "Epoch 17 | Train Loss: 0.7143 Acc: 100.00% | Val Loss: 1.2805 Acc: 58.64%\n",
      "Epoch 18 | Train Loss: 0.7047 Acc: 100.00% | Val Loss: 1.2120 Acc: 58.64%\n",
      "Epoch 19 | Train Loss: 0.7064 Acc: 100.00% | Val Loss: 1.2683 Acc: 57.90%\n",
      "Epoch 20 | Train Loss: 0.7047 Acc: 100.00% | Val Loss: 1.2629 Acc: 47.61%\n",
      "Early stopping at epoch 19\n",
      "Subject 1 Best Acc: 80.70%\n",
      "\n",
      "============================== Processing Subject 2 ==============================\n",
      "Epoch 01 | Train Loss: 0.8956 Acc: 87.61% | Val Loss: 1.1385 Acc: 68.75%\n",
      "Epoch 02 | Train Loss: 0.7807 Acc: 97.40% | Val Loss: 1.1534 Acc: 51.84%\n",
      "Epoch 03 | Train Loss: 0.7615 Acc: 98.11% | Val Loss: 1.1225 Acc: 68.01%\n",
      "Epoch 04 | Train Loss: 0.7583 Acc: 98.11% | Val Loss: 1.1437 Acc: 66.91%\n",
      "Epoch 05 | Train Loss: 0.7479 Acc: 98.73% | Val Loss: 1.2208 Acc: 54.96%\n",
      "Epoch 06 | Train Loss: 0.7470 Acc: 98.67% | Val Loss: 1.2763 Acc: 64.15%\n",
      "Epoch 07 | Train Loss: 0.7488 Acc: 98.47% | Val Loss: 1.3164 Acc: 53.49%\n",
      "Epoch 08 | Train Loss: 0.7265 Acc: 98.98% | Val Loss: 1.2906 Acc: 58.82%\n",
      "Epoch 09 | Train Loss: 0.7221 Acc: 99.29% | Val Loss: 1.3005 Acc: 50.74%\n",
      "Epoch 10 | Train Loss: 0.7247 Acc: 99.13% | Val Loss: 1.3085 Acc: 60.48%\n",
      "Epoch 11 | Train Loss: 0.7218 Acc: 99.18% | Val Loss: 1.3235 Acc: 59.56%\n",
      "Epoch 12 | Train Loss: 0.7156 Acc: 99.69% | Val Loss: 1.2861 Acc: 59.38%\n",
      "Epoch 13 | Train Loss: 0.7156 Acc: 99.54% | Val Loss: 1.3018 Acc: 59.56%\n",
      "Epoch 14 | Train Loss: 0.7140 Acc: 99.75% | Val Loss: 1.3133 Acc: 59.93%\n",
      "Epoch 15 | Train Loss: 0.7108 Acc: 99.54% | Val Loss: 1.2682 Acc: 60.11%\n",
      "Epoch 16 | Train Loss: 0.7100 Acc: 99.75% | Val Loss: 1.2645 Acc: 59.38%\n",
      "Early stopping at epoch 15\n",
      "Subject 2 Best Acc: 68.75%\n",
      "\n",
      "============================== Processing Subject 3 ==============================\n",
      "Epoch 01 | Train Loss: 1.2044 Acc: 59.20% | Val Loss: 1.2796 Acc: 41.91%\n",
      "Epoch 02 | Train Loss: 1.0034 Acc: 77.87% | Val Loss: 1.0837 Acc: 64.34%\n",
      "Epoch 03 | Train Loss: 0.9081 Acc: 87.00% | Val Loss: 1.0663 Acc: 68.75%\n",
      "Epoch 04 | Train Loss: 0.8504 Acc: 92.55% | Val Loss: 1.1669 Acc: 50.18%\n",
      "Epoch 05 | Train Loss: 0.8331 Acc: 94.80% | Val Loss: 1.1517 Acc: 70.59%\n",
      "Epoch 06 | Train Loss: 0.8244 Acc: 93.73% | Val Loss: 1.0294 Acc: 70.40%\n",
      "Epoch 07 | Train Loss: 0.8087 Acc: 95.26% | Val Loss: 1.0692 Acc: 67.83%\n",
      "Epoch 08 | Train Loss: 0.7922 Acc: 96.63% | Val Loss: 1.0759 Acc: 48.90%\n",
      "Epoch 09 | Train Loss: 0.7790 Acc: 97.76% | Val Loss: 1.0986 Acc: 58.82%\n",
      "Epoch 10 | Train Loss: 0.7811 Acc: 97.35% | Val Loss: 1.1371 Acc: 51.10%\n",
      "Epoch 11 | Train Loss: 0.7440 Acc: 99.34% | Val Loss: 1.0442 Acc: 65.26%\n",
      "Epoch 12 | Train Loss: 0.7501 Acc: 99.08% | Val Loss: 1.1195 Acc: 56.99%\n",
      "Epoch 13 | Train Loss: 0.7420 Acc: 99.44% | Val Loss: 1.0792 Acc: 60.85%\n",
      "Epoch 14 | Train Loss: 21.6046 Acc: 42.38% | Val Loss: 1.3804 Acc: 38.24%\n",
      "Epoch 15 | Train Loss: 1.3866 Acc: 25.65% | Val Loss: 1.3836 Acc: 31.99%\n",
      "Epoch 16 | Train Loss: 1.3866 Acc: 24.99% | Val Loss: 1.3870 Acc: 31.99%\n",
      "Epoch 17 | Train Loss: 1.3863 Acc: 24.07% | Val Loss: 1.3888 Acc: 17.28%\n",
      "Epoch 18 | Train Loss: 1.3865 Acc: 24.73% | Val Loss: 1.3883 Acc: 12.50%\n",
      "Epoch 19 | Train Loss: 1.3864 Acc: 24.58% | Val Loss: 1.3864 Acc: 38.24%\n",
      "Epoch 20 | Train Loss: 1.3862 Acc: 25.34% | Val Loss: 1.3860 Acc: 38.24%\n",
      "Early stopping at epoch 19\n",
      "Subject 3 Best Acc: 70.59%\n",
      "\n",
      "============================== Processing Subject 4 ==============================\n",
      "Epoch 01 | Train Loss: 0.9664 Acc: 83.12% | Val Loss: 0.9635 Acc: 74.26%\n",
      "Epoch 02 | Train Loss: 0.7829 Acc: 97.50% | Val Loss: 0.8829 Acc: 84.56%\n",
      "Epoch 03 | Train Loss: 0.7682 Acc: 97.86% | Val Loss: 0.8270 Acc: 95.04%\n",
      "Epoch 04 | Train Loss: 0.7474 Acc: 98.93% | Val Loss: 0.8413 Acc: 92.28%\n",
      "Epoch 05 | Train Loss: 0.7473 Acc: 98.67% | Val Loss: 0.9208 Acc: 83.64%\n",
      "Epoch 06 | Train Loss: 0.7409 Acc: 99.03% | Val Loss: 0.8378 Acc: 88.42%\n",
      "Epoch 07 | Train Loss: 0.7378 Acc: 98.98% | Val Loss: 0.9557 Acc: 76.29%\n",
      "Epoch 08 | Train Loss: 0.7211 Acc: 99.54% | Val Loss: 0.8864 Acc: 79.23%\n",
      "Epoch 09 | Train Loss: 0.7161 Acc: 99.69% | Val Loss: 0.8440 Acc: 90.07%\n",
      "Epoch 10 | Train Loss: 0.7148 Acc: 99.90% | Val Loss: 0.8424 Acc: 92.28%\n",
      "Epoch 11 | Train Loss: 0.7222 Acc: 99.69% | Val Loss: 0.8474 Acc: 87.32%\n",
      "Epoch 12 | Train Loss: 0.7102 Acc: 99.95% | Val Loss: 0.8311 Acc: 92.10%\n",
      "Epoch 13 | Train Loss: 0.7091 Acc: 99.95% | Val Loss: 0.8291 Acc: 87.50%\n",
      "Epoch 14 | Train Loss: 0.7094 Acc: 99.95% | Val Loss: 0.9195 Acc: 77.57%\n",
      "Epoch 15 | Train Loss: 0.7097 Acc: 99.95% | Val Loss: 0.8947 Acc: 80.70%\n",
      "Epoch 16 | Train Loss: 0.7046 Acc: 100.00% | Val Loss: 0.8432 Acc: 90.07%\n",
      "Epoch 17 | Train Loss: 0.7040 Acc: 100.00% | Val Loss: 0.8444 Acc: 86.95%\n",
      "Epoch 18 | Train Loss: 0.7036 Acc: 100.00% | Val Loss: 0.8256 Acc: 93.75%\n",
      "Early stopping at epoch 17\n",
      "Subject 4 Best Acc: 95.04%\n",
      "\n",
      "============================== Processing Subject 5 ==============================\n",
      "Epoch 01 | Train Loss: 1.1639 Acc: 63.23% | Val Loss: 1.2639 Acc: 52.21%\n",
      "Epoch 02 | Train Loss: 0.9129 Acc: 85.52% | Val Loss: 1.1197 Acc: 60.48%\n",
      "Epoch 03 | Train Loss: 14.0620 Acc: 85.42% | Val Loss: 1.5849 Acc: 37.50%\n",
      "Epoch 04 | Train Loss: 1.0423 Acc: 75.78% | Val Loss: 1.1700 Acc: 45.59%\n",
      "Epoch 05 | Train Loss: 0.9492 Acc: 83.78% | Val Loss: 1.1588 Acc: 50.74%\n",
      "Epoch 06 | Train Loss: 0.9239 Acc: 86.28% | Val Loss: 1.2818 Acc: 51.47%\n",
      "Epoch 07 | Train Loss: 0.8658 Acc: 90.46% | Val Loss: 1.1397 Acc: 45.59%\n",
      "Epoch 08 | Train Loss: 0.8584 Acc: 91.48% | Val Loss: 1.2869 Acc: 43.57%\n",
      "Epoch 09 | Train Loss: 0.8413 Acc: 93.12% | Val Loss: 1.1993 Acc: 55.70%\n",
      "Epoch 10 | Train Loss: 0.8349 Acc: 94.24% | Val Loss: 1.2247 Acc: 53.49%\n",
      "Epoch 11 | Train Loss: 0.8055 Acc: 95.46% | Val Loss: 1.2025 Acc: 53.68%\n",
      "Epoch 12 | Train Loss: 0.8026 Acc: 95.05% | Val Loss: 1.1993 Acc: 64.71%\n",
      "Epoch 13 | Train Loss: 0.7986 Acc: 95.77% | Val Loss: 1.2306 Acc: 54.60%\n",
      "Epoch 14 | Train Loss: 0.7864 Acc: 97.04% | Val Loss: 1.1770 Acc: 59.19%\n",
      "Epoch 15 | Train Loss: 0.7711 Acc: 97.50% | Val Loss: 1.1944 Acc: 59.74%\n",
      "Epoch 16 | Train Loss: 0.7674 Acc: 97.45% | Val Loss: 1.2119 Acc: 53.31%\n",
      "Epoch 17 | Train Loss: 0.7840 Acc: 96.02% | Val Loss: 1.1690 Acc: 61.21%\n",
      "Epoch 18 | Train Loss: 0.7707 Acc: 97.09% | Val Loss: 1.1786 Acc: 62.50%\n",
      "Epoch 19 | Train Loss: 0.7563 Acc: 97.55% | Val Loss: 1.2222 Acc: 59.01%\n",
      "Epoch 20 | Train Loss: 0.7557 Acc: 97.25% | Val Loss: 1.1788 Acc: 63.05%\n",
      "Epoch 21 | Train Loss: 0.7500 Acc: 97.96% | Val Loss: 1.2136 Acc: 56.62%\n",
      "Epoch 22 | Train Loss: 0.7582 Acc: 97.40% | Val Loss: 1.2404 Acc: 59.74%\n",
      "Epoch 23 | Train Loss: 0.7509 Acc: 97.40% | Val Loss: 1.2182 Acc: 63.79%\n",
      "Epoch 24 | Train Loss: 0.7483 Acc: 97.50% | Val Loss: 1.2041 Acc: 67.83%\n",
      "Epoch 25 | Train Loss: 0.7422 Acc: 98.27% | Val Loss: 1.2019 Acc: 62.87%\n",
      "Epoch 26 | Train Loss: 0.7515 Acc: 97.35% | Val Loss: 1.1800 Acc: 67.28%\n",
      "Epoch 27 | Train Loss: 0.7354 Acc: 98.11% | Val Loss: 1.2107 Acc: 56.62%\n",
      "Epoch 28 | Train Loss: 0.7384 Acc: 97.76% | Val Loss: 1.2378 Acc: 59.38%\n",
      "Epoch 29 | Train Loss: 0.7377 Acc: 97.96% | Val Loss: 1.2125 Acc: 62.50%\n",
      "Epoch 30 | Train Loss: 0.7321 Acc: 98.67% | Val Loss: 1.1940 Acc: 60.48%\n",
      "Epoch 31 | Train Loss: 0.7310 Acc: 98.52% | Val Loss: 1.2478 Acc: 59.19%\n",
      "Epoch 32 | Train Loss: 0.7346 Acc: 97.86% | Val Loss: 1.2009 Acc: 61.03%\n",
      "Epoch 33 | Train Loss: 0.7333 Acc: 98.16% | Val Loss: 1.2261 Acc: 60.48%\n",
      "Epoch 34 | Train Loss: 0.7306 Acc: 98.37% | Val Loss: 1.2180 Acc: 62.68%\n",
      "Epoch 35 | Train Loss: 0.7274 Acc: 98.88% | Val Loss: 1.2495 Acc: 60.48%\n",
      "Epoch 36 | Train Loss: 0.7314 Acc: 98.47% | Val Loss: 1.2230 Acc: 61.03%\n",
      "Epoch 37 | Train Loss: 0.7248 Acc: 99.08% | Val Loss: 1.2419 Acc: 59.74%\n",
      "Epoch 38 | Train Loss: 0.7276 Acc: 98.83% | Val Loss: 1.1886 Acc: 65.44%\n",
      "Epoch 39 | Train Loss: 0.7237 Acc: 98.67% | Val Loss: 1.2105 Acc: 61.03%\n",
      "Early stopping at epoch 38\n",
      "Subject 5 Best Acc: 67.83%\n",
      "\n",
      "============================== Processing Subject 6 ==============================\n",
      "Epoch 01 | Train Loss: 0.9788 Acc: 80.67% | Val Loss: 1.0821 Acc: 63.79%\n",
      "Epoch 02 | Train Loss: 0.7921 Acc: 96.84% | Val Loss: 0.9756 Acc: 88.24%\n",
      "Epoch 03 | Train Loss: 0.7779 Acc: 96.69% | Val Loss: 1.0558 Acc: 68.57%\n",
      "Epoch 04 | Train Loss: 0.7622 Acc: 97.76% | Val Loss: 1.0015 Acc: 85.11%\n",
      "Epoch 05 | Train Loss: 0.7411 Acc: 99.34% | Val Loss: 0.9102 Acc: 74.08%\n",
      "Epoch 06 | Train Loss: 0.7525 Acc: 98.37% | Val Loss: 1.0566 Acc: 64.15%\n",
      "Epoch 07 | Train Loss: 0.7416 Acc: 98.83% | Val Loss: 1.0083 Acc: 78.31%\n",
      "Epoch 08 | Train Loss: 0.7417 Acc: 98.93% | Val Loss: 1.1152 Acc: 58.46%\n",
      "Epoch 09 | Train Loss: 0.7385 Acc: 98.73% | Val Loss: 0.9716 Acc: 82.90%\n",
      "Epoch 10 | Train Loss: 0.7213 Acc: 99.69% | Val Loss: 0.9838 Acc: 75.55%\n",
      "Epoch 11 | Train Loss: 0.7175 Acc: 99.75% | Val Loss: 0.9805 Acc: 81.07%\n",
      "Epoch 12 | Train Loss: 0.7182 Acc: 99.54% | Val Loss: 1.0909 Acc: 63.05%\n",
      "Epoch 13 | Train Loss: 0.7219 Acc: 99.69% | Val Loss: 0.9821 Acc: 87.68%\n",
      "Epoch 14 | Train Loss: 0.7117 Acc: 99.85% | Val Loss: 0.9708 Acc: 81.07%\n",
      "Epoch 15 | Train Loss: 0.7111 Acc: 99.80% | Val Loss: 0.9967 Acc: 81.07%\n",
      "Epoch 16 | Train Loss: 0.7088 Acc: 99.90% | Val Loss: 0.9448 Acc: 83.64%\n",
      "Epoch 17 | Train Loss: 0.7128 Acc: 99.85% | Val Loss: 0.9216 Acc: 81.07%\n",
      "Early stopping at epoch 16\n",
      "Subject 6 Best Acc: 88.24%\n",
      "\n",
      "============================== Processing Subject 7 ==============================\n",
      "Epoch 01 | Train Loss: 0.9977 Acc: 79.40% | Val Loss: 0.9901 Acc: 82.35%\n",
      "Epoch 02 | Train Loss: 0.8212 Acc: 93.98% | Val Loss: 0.9099 Acc: 92.28%\n",
      "Epoch 03 | Train Loss: 0.7834 Acc: 95.97% | Val Loss: 1.0468 Acc: 63.79%\n",
      "Epoch 04 | Train Loss: 0.7613 Acc: 97.50% | Val Loss: 0.9410 Acc: 92.46%\n",
      "Epoch 05 | Train Loss: 93.9466 Acc: 71.24% | Val Loss: 1.2659 Acc: 40.81%\n",
      "Epoch 06 | Train Loss: 1.1130 Acc: 67.06% | Val Loss: 1.2421 Acc: 42.65%\n",
      "Epoch 07 | Train Loss: 19.6729 Acc: 71.90% | Val Loss: 1.4298 Acc: 17.28%\n",
      "Epoch 08 | Train Loss: 1.3936 Acc: 24.12% | Val Loss: 1.3957 Acc: 17.28%\n",
      "Epoch 09 | Train Loss: 1.3880 Acc: 24.73% | Val Loss: 1.3866 Acc: 38.24%\n",
      "Epoch 10 | Train Loss: 1.3865 Acc: 25.75% | Val Loss: 1.3794 Acc: 38.24%\n",
      "Epoch 11 | Train Loss: 1.3867 Acc: 24.83% | Val Loss: 1.3823 Acc: 38.24%\n",
      "Epoch 12 | Train Loss: 1.3864 Acc: 24.68% | Val Loss: 1.3866 Acc: 12.50%\n",
      "Epoch 13 | Train Loss: 1.3865 Acc: 24.83% | Val Loss: 1.3879 Acc: 12.50%\n",
      "Epoch 14 | Train Loss: 1.3863 Acc: 26.01% | Val Loss: 1.3885 Acc: 12.50%\n",
      "Epoch 15 | Train Loss: 1.3864 Acc: 24.78% | Val Loss: 1.3871 Acc: 12.50%\n",
      "Epoch 16 | Train Loss: 1.3864 Acc: 24.58% | Val Loss: 1.3869 Acc: 12.50%\n",
      "Epoch 17 | Train Loss: 1.3863 Acc: 25.55% | Val Loss: 1.3863 Acc: 12.50%\n",
      "Epoch 18 | Train Loss: 1.3863 Acc: 24.27% | Val Loss: 1.3855 Acc: 31.99%\n",
      "Epoch 19 | Train Loss: 1.3862 Acc: 24.94% | Val Loss: 1.3857 Acc: 12.50%\n",
      "Early stopping at epoch 18\n",
      "Subject 7 Best Acc: 92.46%\n",
      "\n",
      "============================== Processing Subject 8 ==============================\n",
      "Epoch 01 | Train Loss: 1.0051 Acc: 75.93% | Val Loss: 1.1429 Acc: 59.19%\n",
      "Epoch 02 | Train Loss: 66.6049 Acc: 68.84% | Val Loss: 1.3754 Acc: 28.49%\n",
      "Epoch 03 | Train Loss: 1.3174 Acc: 47.32% | Val Loss: 1.3395 Acc: 37.13%\n",
      "Epoch 04 | Train Loss: 1.1905 Acc: 56.25% | Val Loss: 1.3454 Acc: 42.65%\n",
      "Epoch 05 | Train Loss: 1.0805 Acc: 72.31% | Val Loss: 1.3390 Acc: 35.11%\n",
      "Epoch 06 | Train Loss: 0.9956 Acc: 79.50% | Val Loss: 1.4133 Acc: 29.96%\n",
      "Epoch 07 | Train Loss: 0.9629 Acc: 82.20% | Val Loss: 1.3720 Acc: 39.89%\n",
      "Epoch 08 | Train Loss: 0.9371 Acc: 84.24% | Val Loss: 1.3340 Acc: 35.11%\n",
      "Epoch 09 | Train Loss: 0.9175 Acc: 86.89% | Val Loss: 1.2898 Acc: 28.49%\n",
      "Epoch 10 | Train Loss: 0.8976 Acc: 88.78% | Val Loss: 1.2928 Acc: 43.38%\n",
      "Epoch 11 | Train Loss: 0.8911 Acc: 90.01% | Val Loss: 1.3044 Acc: 44.30%\n",
      "Epoch 12 | Train Loss: 0.8882 Acc: 89.70% | Val Loss: 1.3213 Acc: 41.36%\n",
      "Epoch 13 | Train Loss: 0.8802 Acc: 90.97% | Val Loss: 1.2274 Acc: 51.65%\n",
      "Epoch 14 | Train Loss: 0.8589 Acc: 93.01% | Val Loss: 1.2307 Acc: 47.24%\n",
      "Epoch 15 | Train Loss: 0.8635 Acc: 92.55% | Val Loss: 1.2734 Acc: 50.37%\n",
      "Epoch 16 | Train Loss: 0.8611 Acc: 91.99% | Val Loss: 1.2957 Acc: 46.51%\n",
      "Early stopping at epoch 15\n",
      "Subject 8 Best Acc: 59.19%\n",
      "\n",
      "============================== Processing Subject 9 ==============================\n",
      "Epoch 01 | Train Loss: 1.2114 Acc: 59.26% | Val Loss: 1.0488 Acc: 75.74%\n",
      "Epoch 02 | Train Loss: 0.9358 Acc: 85.26% | Val Loss: 1.1242 Acc: 59.19%\n",
      "Epoch 03 | Train Loss: 0.8615 Acc: 90.67% | Val Loss: 1.0400 Acc: 76.29%\n",
      "Epoch 04 | Train Loss: 0.8401 Acc: 92.50% | Val Loss: 1.1358 Acc: 59.38%\n",
      "Epoch 05 | Train Loss: 8.0925 Acc: 93.52% | Val Loss: 53.0603 Acc: 56.25%\n",
      "Epoch 06 | Train Loss: 21.4720 Acc: 47.27% | Val Loss: 1.1928 Acc: 57.17%\n",
      "Epoch 07 | Train Loss: 0.9749 Acc: 79.55% | Val Loss: 1.0065 Acc: 64.15%\n",
      "Epoch 08 | Train Loss: 0.8635 Acc: 90.06% | Val Loss: 1.0033 Acc: 77.76%\n",
      "Epoch 09 | Train Loss: 0.8211 Acc: 94.08% | Val Loss: 0.9707 Acc: 81.80%\n",
      "Epoch 10 | Train Loss: 0.8144 Acc: 93.52% | Val Loss: 0.9106 Acc: 80.15%\n",
      "Epoch 11 | Train Loss: 0.7920 Acc: 95.51% | Val Loss: 0.9919 Acc: 86.76%\n",
      "Epoch 12 | Train Loss: 0.7864 Acc: 96.28% | Val Loss: 0.9835 Acc: 83.09%\n",
      "Epoch 13 | Train Loss: 0.7810 Acc: 97.14% | Val Loss: 0.9960 Acc: 69.49%\n",
      "Epoch 14 | Train Loss: 0.7839 Acc: 96.28% | Val Loss: 0.9555 Acc: 79.60%\n",
      "Epoch 15 | Train Loss: 0.7641 Acc: 97.45% | Val Loss: 0.9682 Acc: 81.07%\n",
      "Epoch 16 | Train Loss: 0.7628 Acc: 98.16% | Val Loss: 0.9312 Acc: 87.87%\n",
      "Epoch 17 | Train Loss: 0.7520 Acc: 98.37% | Val Loss: 0.9421 Acc: 89.52%\n",
      "Epoch 18 | Train Loss: 0.7537 Acc: 98.62% | Val Loss: 0.9278 Acc: 83.09%\n",
      "Epoch 19 | Train Loss: 0.7442 Acc: 98.73% | Val Loss: 0.9284 Acc: 81.25%\n",
      "Epoch 20 | Train Loss: 0.7463 Acc: 98.93% | Val Loss: 0.9278 Acc: 90.26%\n",
      "Epoch 21 | Train Loss: 0.7433 Acc: 98.88% | Val Loss: 0.9189 Acc: 88.05%\n",
      "Epoch 22 | Train Loss: 0.7486 Acc: 98.83% | Val Loss: 0.9366 Acc: 90.26%\n",
      "Epoch 23 | Train Loss: 0.7350 Acc: 99.54% | Val Loss: 0.9349 Acc: 85.11%\n",
      "Epoch 24 | Train Loss: 0.7411 Acc: 99.29% | Val Loss: 0.9318 Acc: 84.74%\n",
      "Epoch 25 | Train Loss: 0.7386 Acc: 99.49% | Val Loss: 0.9281 Acc: 86.03%\n",
      "Epoch 26 | Train Loss: 0.7360 Acc: 99.29% | Val Loss: 0.9443 Acc: 85.11%\n",
      "Epoch 27 | Train Loss: 0.7325 Acc: 99.54% | Val Loss: 0.9593 Acc: 86.58%\n",
      "Epoch 28 | Train Loss: 0.7373 Acc: 99.34% | Val Loss: 0.9294 Acc: 86.21%\n",
      "Epoch 29 | Train Loss: 0.7316 Acc: 99.80% | Val Loss: 0.9293 Acc: 87.13%\n",
      "Epoch 30 | Train Loss: 0.7307 Acc: 99.49% | Val Loss: 0.9310 Acc: 89.89%\n",
      "Epoch 31 | Train Loss: 0.7333 Acc: 99.59% | Val Loss: 0.9391 Acc: 88.79%\n",
      "Epoch 32 | Train Loss: 0.7307 Acc: 99.64% | Val Loss: 0.9392 Acc: 88.24%\n",
      "Epoch 33 | Train Loss: 0.7294 Acc: 99.64% | Val Loss: 0.9402 Acc: 86.40%\n",
      "Epoch 34 | Train Loss: 0.7318 Acc: 99.59% | Val Loss: 0.9448 Acc: 86.40%\n",
      "Epoch 35 | Train Loss: 0.7303 Acc: 99.54% | Val Loss: 0.9406 Acc: 88.97%\n",
      "Early stopping at epoch 34\n",
      "Subject 9 Best Acc: 90.26%\n",
      "\n",
      "============================== Processing Subject 10 ==============================\n",
      "Epoch 01 | Train Loss: 1.0093 Acc: 78.79% | Val Loss: 1.2522 Acc: 64.89%\n",
      "Epoch 02 | Train Loss: 0.8546 Acc: 91.23% | Val Loss: 1.0688 Acc: 70.04%\n",
      "Epoch 03 | Train Loss: 0.8080 Acc: 94.44% | Val Loss: 1.1873 Acc: 64.89%\n",
      "Epoch 04 | Train Loss: 0.7832 Acc: 96.58% | Val Loss: 1.1404 Acc: 64.89%\n",
      "Epoch 05 | Train Loss: 0.7814 Acc: 96.02% | Val Loss: 1.0581 Acc: 66.54%\n",
      "Epoch 06 | Train Loss: 0.7704 Acc: 97.60% | Val Loss: 0.9985 Acc: 75.37%\n",
      "Epoch 07 | Train Loss: 0.7727 Acc: 96.99% | Val Loss: 1.1701 Acc: 61.40%\n",
      "Epoch 08 | Train Loss: 0.7664 Acc: 97.25% | Val Loss: 1.0689 Acc: 64.89%\n",
      "Epoch 09 | Train Loss: 0.7495 Acc: 98.42% | Val Loss: 1.0022 Acc: 71.32%\n",
      "Epoch 10 | Train Loss: 0.7505 Acc: 98.27% | Val Loss: 0.9959 Acc: 64.89%\n",
      "Epoch 11 | Train Loss: 0.7500 Acc: 98.27% | Val Loss: 1.1211 Acc: 67.65%\n",
      "Epoch 12 | Train Loss: 0.7528 Acc: 98.16% | Val Loss: 1.1150 Acc: 64.89%\n",
      "Epoch 13 | Train Loss: 0.7528 Acc: 98.11% | Val Loss: 1.3056 Acc: 61.40%\n",
      "Epoch 14 | Train Loss: 0.7450 Acc: 98.42% | Val Loss: 1.0012 Acc: 73.16%\n",
      "Epoch 15 | Train Loss: 0.7275 Acc: 99.24% | Val Loss: 1.0942 Acc: 61.40%\n",
      "Epoch 16 | Train Loss: 0.7359 Acc: 98.67% | Val Loss: 1.0494 Acc: 64.89%\n",
      "Epoch 17 | Train Loss: 50.7377 Acc: 45.44% | Val Loss: 1.3916 Acc: 17.28%\n",
      "Epoch 18 | Train Loss: 2.1261 Acc: 25.85% | Val Loss: 1.3936 Acc: 17.28%\n",
      "Epoch 19 | Train Loss: 1.3865 Acc: 25.55% | Val Loss: 1.3944 Acc: 12.50%\n",
      "Epoch 20 | Train Loss: 1.3868 Acc: 23.97% | Val Loss: 1.3906 Acc: 12.50%\n",
      "Epoch 21 | Train Loss: 1.3862 Acc: 26.67% | Val Loss: 1.3915 Acc: 12.50%\n",
      "Early stopping at epoch 20\n",
      "Subject 10 Best Acc: 75.37%\n",
      "\n",
      "============================== Processing Subject 11 ==============================\n",
      "Epoch 01 | Train Loss: 1.3707 Acc: 39.98% | Val Loss: 1.4279 Acc: 20.77%\n",
      "Epoch 02 | Train Loss: 21.8464 Acc: 45.84% | Val Loss: 1.5190 Acc: 13.97%\n",
      "Epoch 03 | Train Loss: 1.3056 Acc: 48.95% | Val Loss: 1.3105 Acc: 48.71%\n",
      "Epoch 04 | Train Loss: 1.2423 Acc: 55.63% | Val Loss: 1.4002 Acc: 25.55%\n",
      "Epoch 05 | Train Loss: 1.2119 Acc: 58.54% | Val Loss: 1.2925 Acc: 42.83%\n",
      "Epoch 06 | Train Loss: 1.1803 Acc: 59.66% | Val Loss: 1.4502 Acc: 21.69%\n",
      "Epoch 07 | Train Loss: 1.1905 Acc: 59.66% | Val Loss: 1.2977 Acc: 62.68%\n",
      "Epoch 08 | Train Loss: 1.1823 Acc: 58.08% | Val Loss: 1.2889 Acc: 46.51%\n",
      "Epoch 09 | Train Loss: 1.1508 Acc: 66.14% | Val Loss: 1.2451 Acc: 58.64%\n",
      "Epoch 10 | Train Loss: 1.1520 Acc: 64.00% | Val Loss: 1.3280 Acc: 47.79%\n",
      "Epoch 11 | Train Loss: 1.1419 Acc: 62.47% | Val Loss: 1.4800 Acc: 27.76%\n",
      "Epoch 12 | Train Loss: 40.0758 Acc: 59.46% | Val Loss: 1.3831 Acc: 31.99%\n",
      "Epoch 13 | Train Loss: 1.3695 Acc: 30.85% | Val Loss: 1.3228 Acc: 54.23%\n",
      "Epoch 14 | Train Loss: 1.3366 Acc: 38.30% | Val Loss: 1.2707 Acc: 53.68%\n",
      "Epoch 15 | Train Loss: 1.3335 Acc: 38.40% | Val Loss: 1.3420 Acc: 28.49%\n",
      "Epoch 16 | Train Loss: 1.3240 Acc: 39.21% | Val Loss: 1.3321 Acc: 28.49%\n",
      "Epoch 17 | Train Loss: 1.3126 Acc: 42.78% | Val Loss: 1.3579 Acc: 28.49%\n",
      "Epoch 18 | Train Loss: 1.3099 Acc: 44.57% | Val Loss: 1.3143 Acc: 28.49%\n",
      "Epoch 19 | Train Loss: 1.3068 Acc: 42.63% | Val Loss: 1.2933 Acc: 44.49%\n",
      "Epoch 20 | Train Loss: 1.3133 Acc: 40.08% | Val Loss: 1.2880 Acc: 40.26%\n",
      "Epoch 21 | Train Loss: 1.2941 Acc: 44.62% | Val Loss: 1.2986 Acc: 28.49%\n",
      "Epoch 22 | Train Loss: 1.2983 Acc: 41.97% | Val Loss: 1.3141 Acc: 20.77%\n",
      "Early stopping at epoch 21\n",
      "Subject 11 Best Acc: 62.68%\n",
      "\n",
      "============================== Processing Subject 12 ==============================\n",
      "Epoch 01 | Train Loss: 1.1036 Acc: 72.31% | Val Loss: 1.2402 Acc: 54.60%\n",
      "Epoch 02 | Train Loss: 0.8453 Acc: 93.12% | Val Loss: 1.2285 Acc: 60.85%\n",
      "Epoch 03 | Train Loss: 0.7840 Acc: 97.65% | Val Loss: 1.2620 Acc: 56.07%\n",
      "Epoch 04 | Train Loss: 0.7550 Acc: 98.98% | Val Loss: 1.3260 Acc: 47.06%\n",
      "Epoch 05 | Train Loss: 0.7447 Acc: 99.18% | Val Loss: 1.2207 Acc: 56.07%\n",
      "Epoch 06 | Train Loss: 0.7351 Acc: 99.39% | Val Loss: 1.1729 Acc: 64.15%\n",
      "Epoch 07 | Train Loss: 0.7441 Acc: 99.29% | Val Loss: 1.3515 Acc: 60.29%\n",
      "Epoch 08 | Train Loss: 0.7335 Acc: 99.69% | Val Loss: 1.3143 Acc: 45.77%\n",
      "Epoch 09 | Train Loss: 0.7405 Acc: 99.39% | Val Loss: 1.1951 Acc: 63.42%\n",
      "Epoch 10 | Train Loss: 0.7251 Acc: 99.64% | Val Loss: 1.2414 Acc: 58.82%\n",
      "Epoch 11 | Train Loss: 0.7179 Acc: 100.00% | Val Loss: 1.2959 Acc: 56.80%\n",
      "Epoch 12 | Train Loss: 21.2367 Acc: 71.49% | Val Loss: 1.3827 Acc: 31.99%\n",
      "Epoch 13 | Train Loss: 1.5063 Acc: 25.09% | Val Loss: 1.3821 Acc: 31.99%\n",
      "Epoch 14 | Train Loss: 1.3993 Acc: 23.41% | Val Loss: 1.3862 Acc: 38.24%\n",
      "Epoch 15 | Train Loss: 1.3863 Acc: 26.36% | Val Loss: 1.3823 Acc: 38.24%\n",
      "Epoch 16 | Train Loss: 1.3865 Acc: 24.53% | Val Loss: 1.3800 Acc: 38.24%\n",
      "Epoch 17 | Train Loss: 1.3866 Acc: 25.14% | Val Loss: 1.3795 Acc: 29.96%\n",
      "Epoch 18 | Train Loss: 1.3713 Acc: 29.78% | Val Loss: 1.3809 Acc: 38.24%\n",
      "Epoch 19 | Train Loss: 1.3510 Acc: 31.26% | Val Loss: 1.3895 Acc: 38.24%\n",
      "Epoch 20 | Train Loss: 1.3026 Acc: 43.19% | Val Loss: 1.3886 Acc: 38.24%\n",
      "Epoch 21 | Train Loss: 1.2746 Acc: 43.65% | Val Loss: 1.3997 Acc: 38.24%\n",
      "Early stopping at epoch 20\n",
      "Subject 12 Best Acc: 64.15%\n",
      "\n",
      "============================== Processing Subject 13 ==============================\n",
      "Epoch 01 | Train Loss: 1.0800 Acc: 71.09% | Val Loss: 1.3286 Acc: 46.69%\n",
      "Epoch 02 | Train Loss: 0.8941 Acc: 88.42% | Val Loss: 1.4111 Acc: 32.90%\n",
      "Epoch 03 | Train Loss: 28.3333 Acc: 74.86% | Val Loss: 1.4996 Acc: 9.74%\n",
      "Epoch 04 | Train Loss: 1.0547 Acc: 72.72% | Val Loss: 1.4373 Acc: 36.03%\n",
      "Epoch 05 | Train Loss: 0.9140 Acc: 86.79% | Val Loss: 1.3858 Acc: 35.85%\n",
      "Epoch 06 | Train Loss: 0.8556 Acc: 93.63% | Val Loss: 1.3998 Acc: 31.25%\n",
      "Epoch 07 | Train Loss: 0.8052 Acc: 96.53% | Val Loss: 1.4070 Acc: 35.66%\n",
      "Epoch 08 | Train Loss: 0.7961 Acc: 96.89% | Val Loss: 1.4555 Acc: 34.01%\n",
      "Epoch 09 | Train Loss: 0.7918 Acc: 97.04% | Val Loss: 1.3868 Acc: 37.68%\n",
      "Epoch 10 | Train Loss: 0.7627 Acc: 98.83% | Val Loss: 1.5042 Acc: 39.71%\n",
      "Epoch 11 | Train Loss: 0.7586 Acc: 98.98% | Val Loss: 1.5071 Acc: 43.75%\n",
      "Epoch 12 | Train Loss: 0.7505 Acc: 99.49% | Val Loss: 1.4877 Acc: 33.46%\n",
      "Epoch 13 | Train Loss: 0.7499 Acc: 99.49% | Val Loss: 1.5313 Acc: 42.10%\n",
      "Epoch 14 | Train Loss: 0.7362 Acc: 99.59% | Val Loss: 1.4654 Acc: 37.32%\n",
      "Epoch 15 | Train Loss: 0.7417 Acc: 99.75% | Val Loss: 1.4564 Acc: 38.60%\n",
      "Epoch 16 | Train Loss: 0.7373 Acc: 99.54% | Val Loss: 1.5102 Acc: 41.54%\n",
      "Early stopping at epoch 15\n",
      "Subject 13 Best Acc: 46.69%\n",
      "\n",
      "============================== Processing Subject 14 ==============================\n",
      "Epoch 01 | Train Loss: 1.0803 Acc: 72.62% | Val Loss: 1.1619 Acc: 70.40%\n",
      "Epoch 02 | Train Loss: 0.8519 Acc: 91.28% | Val Loss: 1.2383 Acc: 61.21%\n",
      "Epoch 03 | Train Loss: 0.8123 Acc: 93.78% | Val Loss: 1.1253 Acc: 70.40%\n",
      "Epoch 04 | Train Loss: 0.7921 Acc: 95.61% | Val Loss: 1.1892 Acc: 70.40%\n",
      "Epoch 05 | Train Loss: 0.7779 Acc: 97.30% | Val Loss: 1.1492 Acc: 67.28%\n",
      "Epoch 06 | Train Loss: 0.7655 Acc: 98.11% | Val Loss: 1.1716 Acc: 75.55%\n",
      "Epoch 07 | Train Loss: 0.7560 Acc: 98.06% | Val Loss: 1.1156 Acc: 67.28%\n",
      "Epoch 08 | Train Loss: 0.7575 Acc: 98.01% | Val Loss: 1.1694 Acc: 75.37%\n",
      "Epoch 09 | Train Loss: 0.7534 Acc: 98.37% | Val Loss: 1.1144 Acc: 73.90%\n",
      "Epoch 10 | Train Loss: 29.0142 Acc: 77.15% | Val Loss: 1.4633 Acc: 65.62%\n",
      "Epoch 11 | Train Loss: 0.8715 Acc: 90.21% | Val Loss: 1.1986 Acc: 74.45%\n",
      "Epoch 12 | Train Loss: 0.8357 Acc: 93.78% | Val Loss: 1.3190 Acc: 58.27%\n",
      "Epoch 13 | Train Loss: 0.8148 Acc: 95.36% | Val Loss: 1.3868 Acc: 55.33%\n",
      "Epoch 14 | Train Loss: 0.7808 Acc: 97.71% | Val Loss: 1.2975 Acc: 65.81%\n",
      "Epoch 15 | Train Loss: 0.7746 Acc: 98.27% | Val Loss: 1.2274 Acc: 74.45%\n",
      "Epoch 16 | Train Loss: 0.7632 Acc: 98.78% | Val Loss: 1.2320 Acc: 55.88%\n",
      "Epoch 17 | Train Loss: 0.7651 Acc: 98.83% | Val Loss: 1.2095 Acc: 56.80%\n",
      "Epoch 18 | Train Loss: 0.7567 Acc: 99.03% | Val Loss: 1.2233 Acc: 74.45%\n",
      "Epoch 19 | Train Loss: 0.7453 Acc: 99.54% | Val Loss: 1.2717 Acc: 71.51%\n",
      "Epoch 20 | Train Loss: 0.7458 Acc: 99.69% | Val Loss: 1.1766 Acc: 79.04%\n",
      "Epoch 21 | Train Loss: 0.7432 Acc: 99.85% | Val Loss: 1.2129 Acc: 76.47%\n",
      "Epoch 22 | Train Loss: 0.7344 Acc: 99.80% | Val Loss: 1.2066 Acc: 76.84%\n",
      "Epoch 23 | Train Loss: 0.7361 Acc: 99.80% | Val Loss: 1.2753 Acc: 75.00%\n",
      "Epoch 24 | Train Loss: 0.7404 Acc: 99.80% | Val Loss: 1.2364 Acc: 69.49%\n",
      "Epoch 25 | Train Loss: 0.7350 Acc: 99.85% | Val Loss: 1.2479 Acc: 74.45%\n",
      "Epoch 26 | Train Loss: 0.7307 Acc: 99.75% | Val Loss: 1.2270 Acc: 78.49%\n",
      "Epoch 27 | Train Loss: 0.7297 Acc: 99.80% | Val Loss: 1.2415 Acc: 74.45%\n",
      "Epoch 28 | Train Loss: 0.7265 Acc: 99.80% | Val Loss: 1.2219 Acc: 74.45%\n",
      "Epoch 29 | Train Loss: 0.7315 Acc: 99.75% | Val Loss: 1.2840 Acc: 70.40%\n",
      "Epoch 30 | Train Loss: 0.7274 Acc: 99.95% | Val Loss: 1.2710 Acc: 72.24%\n",
      "Epoch 31 | Train Loss: 0.7249 Acc: 100.00% | Val Loss: 1.2519 Acc: 74.45%\n",
      "Epoch 32 | Train Loss: 0.7251 Acc: 99.90% | Val Loss: 1.2513 Acc: 74.45%\n",
      "Epoch 33 | Train Loss: 0.7232 Acc: 100.00% | Val Loss: 1.2480 Acc: 75.55%\n",
      "Epoch 34 | Train Loss: 0.7228 Acc: 100.00% | Val Loss: 1.2699 Acc: 74.45%\n",
      "Epoch 35 | Train Loss: 0.7224 Acc: 100.00% | Val Loss: 1.2651 Acc: 74.45%\n",
      "Early stopping at epoch 34\n",
      "Subject 14 Best Acc: 79.04%\n",
      "\n",
      "============================== Processing Subject 15 ==============================\n",
      "Epoch 01 | Train Loss: 0.9332 Acc: 85.77% | Val Loss: 0.8794 Acc: 84.38%\n",
      "Epoch 02 | Train Loss: 0.7465 Acc: 98.93% | Val Loss: 0.7889 Acc: 96.32%\n",
      "Epoch 03 | Train Loss: 0.7342 Acc: 99.24% | Val Loss: 0.7807 Acc: 93.75%\n",
      "Epoch 04 | Train Loss: 0.7226 Acc: 99.69% | Val Loss: 0.7998 Acc: 93.75%\n",
      "Epoch 05 | Train Loss: 0.7215 Acc: 99.75% | Val Loss: 0.7644 Acc: 93.75%\n",
      "Epoch 06 | Train Loss: 0.7227 Acc: 99.49% | Val Loss: 0.8084 Acc: 88.60%\n",
      "Epoch 07 | Train Loss: 0.7185 Acc: 99.59% | Val Loss: 0.7722 Acc: 93.75%\n",
      "Epoch 08 | Train Loss: 0.7178 Acc: 99.85% | Val Loss: 0.7865 Acc: 93.75%\n",
      "Epoch 09 | Train Loss: 0.7194 Acc: 99.39% | Val Loss: 0.7540 Acc: 100.00%\n",
      "Epoch 10 | Train Loss: 0.7176 Acc: 99.59% | Val Loss: 0.7591 Acc: 96.88%\n",
      "Epoch 11 | Train Loss: 0.7136 Acc: 99.90% | Val Loss: 0.7497 Acc: 99.26%\n",
      "Epoch 12 | Train Loss: 0.7150 Acc: 99.69% | Val Loss: 0.8123 Acc: 88.60%\n",
      "Epoch 13 | Train Loss: 0.7145 Acc: 99.90% | Val Loss: 0.7784 Acc: 93.75%\n",
      "Epoch 14 | Train Loss: 0.7154 Acc: 99.85% | Val Loss: 0.7815 Acc: 93.75%\n",
      "Epoch 15 | Train Loss: 0.7133 Acc: 99.95% | Val Loss: 0.7629 Acc: 100.00%\n",
      "Epoch 16 | Train Loss: 0.7076 Acc: 99.95% | Val Loss: 0.7681 Acc: 98.71%\n",
      "Epoch 17 | Train Loss: 0.7062 Acc: 100.00% | Val Loss: 0.7758 Acc: 95.22%\n",
      "Epoch 18 | Train Loss: 0.7055 Acc: 99.95% | Val Loss: 0.7995 Acc: 90.99%\n",
      "Epoch 19 | Train Loss: 0.7057 Acc: 100.00% | Val Loss: 0.8052 Acc: 92.10%\n",
      "Epoch 20 | Train Loss: 0.7025 Acc: 100.00% | Val Loss: 0.7853 Acc: 93.75%\n",
      "Epoch 21 | Train Loss: 0.7028 Acc: 100.00% | Val Loss: 0.7855 Acc: 93.75%\n",
      "Epoch 22 | Train Loss: 0.7030 Acc: 100.00% | Val Loss: 0.8008 Acc: 93.75%\n",
      "Epoch 23 | Train Loss: 104.0987 Acc: 67.06% | Val Loss: 10.9387 Acc: 12.50%\n",
      "Epoch 24 | Train Loss: 1.4510 Acc: 25.80% | Val Loss: 1.3977 Acc: 17.28%\n",
      "Early stopping at epoch 23\n",
      "Subject 15 Best Acc: 100.00%\n",
      "\n",
      "========================================\n",
      "FINAL RESULTS REPORT \n",
      "========================================\n",
      "Subject 1: 80.70%\n",
      "Subject 2: 68.75%\n",
      "Subject 3: 70.59%\n",
      "Subject 4: 95.04%\n",
      "Subject 5: 67.83%\n",
      "Subject 6: 88.24%\n",
      "Subject 7: 92.46%\n",
      "Subject 8: 59.19%\n",
      "Subject 9: 90.26%\n",
      "Subject 10: 75.37%\n",
      "Subject 11: 62.68%\n",
      "Subject 12: 64.15%\n",
      "Subject 13: 46.69%\n",
      "Subject 14: 79.04%\n",
      "Subject 15: 100.00%\n",
      "\n",
      "Average Accuracy: 76.07%\n",
      "HIGHEST ACCURACY: Subject 15 (100.00%)\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Subject-Dependent Loop\n",
    "\n",
    "# Create a directory to save the best models\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "subject_results = {}\n",
    "\n",
    "print(\"Start Subject-Dependent Evaluation...\")\n",
    "\n",
    "for subject_id in all_subjects:\n",
    "    print(f\"\\n{'='*30} Processing Subject {subject_id} {'='*30}\")\n",
    "    \n",
    "    # Split Data (Train/Test) by Trial ID \n",
    "    subject_df = meta_info[meta_info['subject_id'] == subject_id]\n",
    "    all_trials = subject_df['trial_id'].unique().tolist()\n",
    "    \n",
    "    # Pick 5 random videos for testing \n",
    "    random.seed(42) \n",
    "    test_trials = random.sample(all_trials, 5)\n",
    "    train_trials = [t for t in all_trials if t not in test_trials]\n",
    "    \n",
    "    # Get Indices\n",
    "    train_indices = subject_df[subject_df['trial_id'].isin(train_trials)].index.tolist()\n",
    "    test_indices = subject_df[subject_df['trial_id'].isin(test_trials)].index.tolist()\n",
    "    \n",
    "    # Create Subsets\n",
    "    train_set = Subset(dataset, train_indices)\n",
    "    test_set = Subset(dataset, test_indices)\n",
    "    \n",
    "    # Weighted Sampler (To balance emotions)\n",
    "    y_train = meta_info.iloc[train_indices]['emotion'].values\n",
    "    class_counts = np.bincount(y_train)\n",
    "    class_weights = 1. / (class_counts + 1e-6)\n",
    "    sample_weights = [class_weights[y] for y in y_train]\n",
    "    \n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    \n",
    "    # Loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, sampler=sampler)\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # --- 3. Initialize Model ---\n",
    "    model = DGCNN(\n",
    "        in_channels=5, \n",
    "        num_electrodes=62, \n",
    "        num_layers=2, \n",
    "        hid_channels=HIDE_CHANNELS, \n",
    "        num_classes=4,\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss( label_smoothing = LABEL_SMOOTHING)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=REDUCE_FACTOR, patience=PATIENCE_LR)\n",
    "   \n",
    "    # Training Loop \n",
    "    best_val_acc = 0.0\n",
    "    counter = 0 # For early stopping\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device).long()\n",
    "            \n",
    "            if len(X.shape) == 4: X = X.squeeze(1) \n",
    "          \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            \n",
    "       \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device).long()\n",
    "                if len(X.shape) == 4: X = X.squeeze(1)\n",
    "                \n",
    "                outputs = model(X)\n",
    "                val_loss += criterion(outputs, y).item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += y.size(0)\n",
    "                val_correct += (predicted == y).sum().item()\n",
    "        \n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Early Stopping Check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "\n",
    "            save_path = f\"saved_models/subject_{subject_id}_best.pth\"\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            \n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= PATIENCE_ES:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "    print(f\"Subject {subject_id} Best Acc: {best_val_acc:.2f}%\")\n",
    "    subject_results[subject_id] = best_val_acc\n",
    "\n",
    "# ================= FINAL REPORT =================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FINAL RESULTS REPORT \")\n",
    "print(\"=\"*40)\n",
    "\n",
    "all_accuracies = list(subject_results.values())\n",
    "\n",
    "for sub_id in sorted(subject_results.keys()):\n",
    "    print(f\"Subject {sub_id}: {subject_results[sub_id]:.2f}%\")\n",
    "\n",
    "print(f\"\\nAverage Accuracy: {np.mean(all_accuracies):.2f}%\")\n",
    "\n",
    "if len(subject_results) > 0:\n",
    "    best_sub_id = max(subject_results, key=subject_results.get)\n",
    "    print(f\"HIGHEST ACCURACY: Subject {best_sub_id} ({subject_results[best_sub_id]:.2f}%)\")\n",
    "print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 218098,
     "sourceId": 472319,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1449.913923,
   "end_time": "2025-12-11T10:53:51.079819",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-11T10:29:41.165896",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
