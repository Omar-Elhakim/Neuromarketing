{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":472319,"sourceType":"datasetVersion","datasetId":218098}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch_scatter torcheeg torch_geometric -qq ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:30:04.880392Z","iopub.execute_input":"2025-12-05T14:30:04.880557Z","iopub.status.idle":"2025-12-05T14:44:01.250909Z","shell.execute_reply.started":"2025-12-05T14:30:04.880544Z","shell.execute_reply":"2025-12-05T14:44:01.250110Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m251.4/251.4 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m231.5/231.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m295.1/295.1 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m750.2/750.2 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for torcheeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for spectrum (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.18.0 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\njax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nscikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncvxpy 1.6.7 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nxarray-einstats 0.9.1 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\njaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#!rm -rf tmp_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:44:01.252295Z","iopub.execute_input":"2025-12-05T14:44:01.252529Z","iopub.status.idle":"2025-12-05T14:44:01.256228Z","shell.execute_reply.started":"2025-12-05T14:44:01.252505Z","shell.execute_reply":"2025-12-05T14:44:01.255549Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset, Dataset, WeightedRandomSampler\nfrom torcheeg.datasets import SEEDIVDataset\nfrom torcheeg import transforms\nimport scipy.signal as signal\nimport numpy as np\nimport random\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport copy\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:44:01.257092Z","iopub.execute_input":"2025-12-05T14:44:01.257373Z","iopub.status.idle":"2025-12-05T14:44:05.983236Z","shell.execute_reply.started":"2025-12-05T14:44:01.257346Z","shell.execute_reply":"2025-12-05T14:44:05.982660Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Set device to GPU if available, otherwise CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:44:05.985015Z","iopub.execute_input":"2025-12-05T14:44:05.985435Z","iopub.status.idle":"2025-12-05T14:44:06.017735Z","shell.execute_reply.started":"2025-12-05T14:44:05.985417Z","shell.execute_reply":"2025-12-05T14:44:06.016671Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Set a fixed random seed for reproducibility across different libraries.\ndef set_seed(seed_value=42):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed_value)\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:44:06.018698Z","iopub.execute_input":"2025-12-05T14:44:06.019057Z","iopub.status.idle":"2025-12-05T14:44:06.042329Z","shell.execute_reply.started":"2025-12-05T14:44:06.019037Z","shell.execute_reply":"2025-12-05T14:44:06.041557Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def BandPassFilter(eeg_data):\n    b, a = signal.butter(4, Wn=[1.0, 75.0], btype='bandpass', fs=200)\n    return signal.filtfilt(b, a, eeg_data, axis=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:44:06.043925Z","iopub.execute_input":"2025-12-05T14:44:06.044130Z","iopub.status.idle":"2025-12-05T14:44:06.048047Z","shell.execute_reply.started":"2025-12-05T14:44:06.044115Z","shell.execute_reply":"2025-12-05T14:44:06.047364Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def Notch(eeg_data):\n    b, a = signal.iirnotch(w0=50.0, Q=30.0, fs=200)\n    return signal.filtfilt(b, a, eeg_data, axis=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:44:06.048779Z","iopub.execute_input":"2025-12-05T14:44:06.049146Z","iopub.status.idle":"2025-12-05T14:44:06.060670Z","shell.execute_reply.started":"2025-12-05T14:44:06.049103Z","shell.execute_reply":"2025-12-05T14:44:06.060069Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# 2. Define Preprocessing\nt_transform = transforms.Compose([\n    transforms.Lambda(BandPassFilter),\n    transforms.Lambda(Notch),\n    transforms.BaselineRemoval(),\n    transforms.MeanStdNormalize(),\n    transforms.To2d()\n    \n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:44:06.061416Z","iopub.execute_input":"2025-12-05T14:44:06.061604Z","iopub.status.idle":"2025-12-05T14:44:06.073236Z","shell.execute_reply.started":"2025-12-05T14:44:06.061589Z","shell.execute_reply":"2025-12-05T14:44:06.072505Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"dataset = SEEDIVDataset(\n    io_path='./tmp_out/seed_iv',\n    root_path='/kaggle/input/seed-iv/eeg_raw_data',\n    offline_transform=t_transform,\n    label_transform=transforms.Select('emotion'),\n    chunk_size=800,  # 4 seconds\n    num_worker=0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:44:06.074028Z","iopub.execute_input":"2025-12-05T14:44:06.074515Z","iopub.status.idle":"2025-12-05T14:52:05.897252Z","shell.execute_reply.started":"2025-12-05T14:44:06.074491Z","shell.execute_reply":"2025-12-05T14:52:05.896654Z"}},"outputs":[{"name":"stderr","text":"[2025-12-05 14:44:06] INFO (torcheeg/MainThread) üîç | Processing EEG data. Processed EEG data has been cached to \u001b[92m./tmp_out/seed_iv\u001b[0m.\n[2025-12-05 14:44:06] INFO (torcheeg/MainThread) ‚è≥ | Monitoring the detailed processing of a record for debugging. The processing of other records will only be reported in percentage to keep it clean.\n[PROCESS]:   0%|          | 0/45 [00:00<?, ?it/s]\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 0it [00:00, ?it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 1it [00:02,  2.47s/it]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 11it [00:02,  5.84it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 21it [00:02, 12.64it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 32it [00:02, 21.56it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 42it [00:02, 30.67it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 53it [00:02, 41.60it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 64it [00:03, 52.73it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 75it [00:03, 63.36it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 86it [00:03, 72.65it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 97it [00:03, 80.49it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 108it [00:03, 85.75it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 119it [00:03, 91.23it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 130it [00:03, 94.59it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 141it [00:03, 97.71it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 152it [00:03, 98.40it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 163it [00:04, 99.39it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 174it [00:04, 97.92it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 185it [00:04, 94.75it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 196it [00:04, 97.31it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 207it [00:04, 98.85it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 217it [00:04, 98.81it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 228it [00:04, 100.40it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 239it [00:04, 100.71it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 250it [00:04, 98.19it/s] \u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 260it [00:05, 95.12it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 270it [00:05, 36.65it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 281it [00:05, 45.81it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 292it [00:05, 55.05it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 303it [00:06, 63.96it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 314it [00:06, 72.78it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 325it [00:06, 80.39it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 336it [00:06, 86.43it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 347it [00:06, 91.63it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 358it [00:06, 95.50it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 369it [00:06, 98.26it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 380it [00:06, 99.82it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 391it [00:06, 101.22it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 402it [00:07, 102.18it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 413it [00:07, 101.68it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 424it [00:07, 101.80it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 435it [00:07, 101.69it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 446it [00:07, 101.73it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 457it [00:07, 102.71it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 468it [00:07, 99.79it/s] \u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 479it [00:07, 99.91it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 490it [00:07, 100.43it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 501it [00:07, 100.92it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 512it [00:08, 101.90it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 523it [00:08, 101.14it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 534it [00:08, 100.76it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 545it [00:08, 99.94it/s] \u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 556it [00:08, 99.61it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 566it [00:08, 99.12it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 577it [00:08, 99.62it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 587it [00:08, 98.80it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 597it [00:08, 98.87it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 607it [00:09, 98.78it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 618it [00:09, 99.95it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 629it [00:09, 100.29it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 640it [00:09, 101.00it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 651it [00:09, 102.40it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 662it [00:09, 103.47it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 673it [00:09, 102.97it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 684it [00:09, 101.31it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 695it [00:09, 96.23it/s] \u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 705it [00:10, 93.71it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 715it [00:10, 32.75it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 722it [00:11, 23.66it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 732it [00:11, 30.82it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 743it [00:11, 40.35it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 754it [00:11, 50.38it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 764it [00:11, 58.96it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 775it [00:11, 68.07it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 786it [00:12, 75.84it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 797it [00:12, 82.54it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 807it [00:12, 86.41it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 818it [00:12, 90.76it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 828it [00:12, 92.25it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 839it [00:12, 95.38it/s]\u001b[A\n[RECORD /kaggle/input/seed-iv/eeg_raw_data/1/4_20151111.mat]: 850it [00:12, 97.33it/s]\u001b[A\n[PROCESS]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [07:59<00:00, 10.66s/it]                            \u001b[A\n[2025-12-05 14:52:05] INFO (torcheeg/MainThread) ‚úÖ | All processed EEG data has been cached to ./tmp_out/seed_iv.\n[2025-12-05 14:52:05] INFO (torcheeg/MainThread) üòä | Please set \u001b[92mio_path\u001b[0m to \u001b[92m./tmp_out/seed_iv\u001b[0m for the next run, to directly read from the cache if you wish to skip the data processing step.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"BATCH_SIZE = 64         \nMAX_EPOCHS = 100\nPATIENCE = 15                \nSCHED_FACTOR = 0.3\nSCHED_PATIENCE = 5\nLEARNING_RATE = 5e-4\nWEIGHT_DECAY = 5e-4\nCLIP_GRAD = 1.0  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:52:05.899973Z","iopub.execute_input":"2025-12-05T14:52:05.900203Z","iopub.status.idle":"2025-12-05T14:52:05.904373Z","shell.execute_reply.started":"2025-12-05T14:52:05.900186Z","shell.execute_reply":"2025-12-05T14:52:05.903722Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"df = dataset.info\ncounts = df['emotion'].value_counts().sort_index()\nprint(\"Class Counts:\\n\", counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:52:05.905024Z","iopub.execute_input":"2025-12-05T14:52:05.905239Z","iopub.status.idle":"2025-12-05T14:52:05.927239Z","shell.execute_reply.started":"2025-12-05T14:52:05.905225Z","shell.execute_reply":"2025-12-05T14:52:05.926649Z"}},"outputs":[{"name":"stdout","text":"Class Counts:\n emotion\n0    10170\n1    10245\n2     9225\n3     7935\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Split by Trial ID\n# SEED-IV has 24 trials (videos) per session.\n# 80% of VIDEOS for training (19 videos), 20% for testing (5 videos).\nall_trial_ids = list(range(1, 25))\n\nrandom.seed(42)\ntest_trial_ids = random.sample(all_trial_ids, 5)\ntrain_trial_ids = [t for t in all_trial_ids if t not in test_trial_ids]\n\ntrain_indices = df[df['trial_id'].isin(train_trial_ids)].index.tolist()\ntest_indices = df[df['trial_id'].isin(test_trial_ids)].index.tolist()\n\n# Create Subsets & Loaders\ntrain_set = Subset(dataset, train_indices)\ntest_set = Subset(dataset, test_indices)\n\n#train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n#test_loader = DataLoader(test_set, batch_size=64, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:25:56.195177Z","iopub.execute_input":"2025-12-05T15:25:56.195488Z","iopub.status.idle":"2025-12-05T15:25:56.207252Z","shell.execute_reply.started":"2025-12-05T15:25:56.195468Z","shell.execute_reply":"2025-12-05T15:25:56.206483Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"labels = df[\"emotion\"].values\n\ntrain_labels = labels[train_indices]\n\nclass_counts = np.bincount(train_labels)\nclass_weights = 1.0 / class_counts\nsample_weights = class_weights[train_labels]\n\nsampler = WeightedRandomSampler(\n    weights=torch.DoubleTensor(sample_weights),\n    num_samples=len(sample_weights),\n    replacement=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:26:00.923929Z","iopub.execute_input":"2025-12-05T15:26:00.924212Z","iopub.status.idle":"2025-12-05T15:26:00.940051Z","shell.execute_reply.started":"2025-12-05T15:26:00.924192Z","shell.execute_reply":"2025-12-05T15:26:00.939203Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"train_loader = DataLoader(\n    Subset(dataset, train_indices),\n    batch_size=BATCH_SIZE,\n    sampler=sampler,\n    num_workers=0,\n    pin_memory=(device.type == \"cuda\")\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:26:03.970954Z","iopub.execute_input":"2025-12-05T15:26:03.971555Z","iopub.status.idle":"2025-12-05T15:26:03.975345Z","shell.execute_reply.started":"2025-12-05T15:26:03.971534Z","shell.execute_reply":"2025-12-05T15:26:03.974636Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"test_loader = DataLoader(\n    Subset(dataset, test_indices),\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=0,\n    pin_memory=(device.type == \"cuda\")\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:26:06.902332Z","iopub.execute_input":"2025-12-05T15:26:06.902922Z","iopub.status.idle":"2025-12-05T15:26:06.906631Z","shell.execute_reply.started":"2025-12-05T15:26:06.902901Z","shell.execute_reply":"2025-12-05T15:26:06.905754Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class EEG_CNN(nn.Module):\n    \n    def __init__(self, in_channels=1, num_classes=4):\n        super().__init__()\n\n        self.print_dims = True\n\n        # ----- Block 1: Temporal Conv (Preserve Spatial, Reduce Temporal) -----\n        # 62x800 -> 62x(800 - 30 + 30) = 62x800\n        self.temporal_conv = nn.Sequential(\n            nn.ZeroPad2d((15, 15, 0, 0)), \n            nn.Conv2d(in_channels=1, out_channels=20, kernel_size=(1, 31), stride=(1, 1), padding=0),\n            nn.BatchNorm2d(20, affine=False),\n            nn.LeakyReLU(),\n            nn.Dropout(p=0.4) \n        )\n\n        # ----- Block 2: Spatial Conv (Reduce Spatial, Reduce Temporal slightly) -----\n        # Input: [B, 20, 62, 800]\n        # kernel_size=(62, 1) \n        \n        self.spatial_conv = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=40, kernel_size=(62, 1), stride=(1, 1), padding=0), \n            nn.LeakyReLU(),\n            nn.Dropout(p=0.4)\n        )\n\n        # ----- Block 3: Temporal Downsampling (Pooling) -----\n        # Input: [B, 40, 1, 800]\n        # MaxPool2d(kernel=(1, 4), stride=(1, 4))\n        self.temporal_pool1 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4)), # 800 -> 200\n            nn.BatchNorm2d(40, affine=False),\n            nn.LeakyReLU()\n        )\n\n        # ----- Block 4: Deep Temporal Feature Extraction -----\n        # Input: [B, 40, 1, 200]\n        self.deep_conv = nn.Sequential(\n            nn.Conv2d(in_channels=40, out_channels=80, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5)), # 200\n            nn.BatchNorm2d(80, affine=False),\n            nn.LeakyReLU(),\n            nn.Dropout(p=0.4), \n            \n            nn.Conv2d(in_channels=80, out_channels=160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3)), # 200\n            nn.BatchNorm2d(160, affine=False),\n            nn.LeakyReLU(),\n            nn.Dropout(p=0.4)\n        )\n        \n        # ----- Global Pooling & Classification -----\n        # Input: [B, 160, 1, 200]\n        self.gap = nn.AdaptiveAvgPool2d((1, 1)) # [B, 160, 1, 1]\n\n        self.classifier = nn.Sequential(\n            nn.Flatten(), # [B, 160]\n            nn.Linear(160, num_classes)\n        )\n\n    def forward(self, x):\n        \n        if self.print_dims: print(\"Input:\", x.shape) # [B, 1, 62, 800]\n\n        # Block 1\n        x = self.temporal_conv(x)\n        if self.print_dims: print(\"After Temporal Conv (B1):\", x.shape) # [B, 20, 62, 800]\n\n        # Block 2\n        x = self.spatial_conv(x)\n        if self.print_dims: print(\"After Spatial Conv (B2):\", x.shape) # [B, 40, 1, 800]\n\n        # Block 3\n        x = self.temporal_pool1(x)\n        if self.print_dims: print(\"After Temporal Pool (B3):\", x.shape) # [B, 40, 1, 200]\n            \n        # Block 4\n        x = self.deep_conv(x)\n        if self.print_dims: print(\"After Deep Conv (B4):\", x.shape) # [B, 160, 1, 200]\n\n        # Global Pooling\n        x = self.gap(x) \n        if self.print_dims: print(\"After Global Avg Pool (GAP):\", x.shape) # [B, 160, 1, 1]\n        \n        if self.print_dims:\n            self.print_dims = False\n        \n        x = self.classifier(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:26:11.061016Z","iopub.execute_input":"2025-12-05T15:26:11.061721Z","iopub.status.idle":"2025-12-05T15:26:11.071480Z","shell.execute_reply.started":"2025-12-05T15:26:11.061698Z","shell.execute_reply":"2025-12-05T15:26:11.070603Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n\n\ndef train_epoch(model, loader, criterion, optimizer, clip_grad=None):\n    model.train()\n    loss_sum = 0.0\n    correct = 0\n    total = 0\n\n    for X, y in loader:\n        X = X.to(device).float()\n        y = y.to(device).long()\n\n        optimizer.zero_grad()\n        \n        with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n            out = model(X)\n            loss = criterion(out, y)\n\n        scaler.scale(loss).backward()\n\n        if clip_grad:\n            # unscale then clip\n            scaler.unscale_(optimizer)\n            nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n\n        scaler.step(optimizer)\n        scaler.update()\n\n        batch = y.size(0)\n        loss_sum += loss.item() * batch\n        correct += (out.argmax(1) == y).sum().item()\n        total += batch\n\n    return loss_sum / total, 100.0 * correct / total\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:26:15.383196Z","iopub.execute_input":"2025-12-05T15:26:15.383689Z","iopub.status.idle":"2025-12-05T15:26:15.390251Z","shell.execute_reply.started":"2025-12-05T15:26:15.383668Z","shell.execute_reply":"2025-12-05T15:26:15.389460Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_81/4178704967.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def evaluate(model, loader, criterion):\n    model.eval()\n    loss_sum = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for X, y in loader:\n            X = X.to(device).float()\n            y = y.to(device).long()\n            with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n                out = model(X)\n                loss = criterion(out, y)\n\n            batch = y.size(0)\n            loss_sum += loss.item() * batch\n            correct += (out.argmax(1) == y).sum().item()\n            total += batch\n\n    return loss_sum / total, 100.0 * correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:26:21.367057Z","iopub.execute_input":"2025-12-05T15:26:21.367632Z","iopub.status.idle":"2025-12-05T15:26:21.373014Z","shell.execute_reply.started":"2025-12-05T15:26:21.367592Z","shell.execute_reply":"2025-12-05T15:26:21.372140Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def train_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, scheduler,\n                              max_epochs=MAX_EPOCHS, patience=PATIENCE, clip_grad=CLIP_GRAD):\n    best_acc = 0.0\n    best_state = None\n    counter = 0\n\n    for epoch in range(max_epochs):\n        # free small fragments each epoch \n        if device.type == 'cuda':\n            torch.cuda.empty_cache()\n            \n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, clip_grad=clip_grad)\n        val_loss, val_acc = evaluate(model, val_loader, criterion)\n\n        scheduler.step(val_loss)\n\n        print(f\"Epoch {epoch+1:02d} | Train={train_acc:.2f}% | Val={val_acc:.2f}% | ValLoss={val_loss:.4f}\")\n\n        if val_acc > best_acc + 1e-4:   \n            best_acc = val_acc\n            best_state = copy.deepcopy(model.state_dict())\n            counter = 0\n        else:\n            counter += 1\n            if counter >= patience:\n                print(f\"Early stopping (no improvement for {patience} epochs).\")\n                break\n\n    return best_acc, best_state\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:26:27.941088Z","iopub.execute_input":"2025-12-05T15:26:27.941827Z","iopub.status.idle":"2025-12-05T15:26:27.947243Z","shell.execute_reply.started":"2025-12-05T15:26:27.941802Z","shell.execute_reply":"2025-12-05T15:26:27.946580Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# ------------------ Prepare model, optimizer, scheduler ------------------\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nmodel = EEG_CNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=SCHED_FACTOR, patience=SCHED_PATIENCE, verbose=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:26:32.279960Z","iopub.execute_input":"2025-12-05T15:26:32.280446Z","iopub.status.idle":"2025-12-05T15:26:35.111020Z","shell.execute_reply.started":"2025-12-05T15:26:32.280424Z","shell.execute_reply":"2025-12-05T15:26:35.110273Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ------------------ Run training ------------------\nbest_acc, best_state = train_with_early_stopping(\n    model, train_loader, test_loader, criterion, optimizer, scheduler,\n    max_epochs=MAX_EPOCHS, patience=PATIENCE, clip_grad=CLIP_GRAD\n)\n\nif best_state is not None:\n    model.load_state_dict(best_state)\n\ntest_loss, test_acc = evaluate(model, test_loader, criterion)\nprint(f\"\\nFinal Test Accuracy: {test_acc:.2f}%  (best seen during training: {best_acc:.2f}%)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:27:13.921707Z","iopub.execute_input":"2025-12-05T15:27:13.921987Z","iopub.status.idle":"2025-12-05T16:17:49.698066Z","shell.execute_reply.started":"2025-12-05T15:27:13.921968Z","shell.execute_reply":"2025-12-05T16:17:49.697281Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_81/4178704967.py:16: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n","output_type":"stream"},{"name":"stdout","text":"Input: torch.Size([64, 1, 62, 800])\nAfter Temporal Conv (B1): torch.Size([64, 20, 62, 800])\nAfter Spatial Conv (B2): torch.Size([64, 40, 1, 800])\nAfter Temporal Pool (B3): torch.Size([64, 40, 1, 200])\nAfter Deep Conv (B4): torch.Size([64, 160, 1, 200])\nAfter Global Avg Pool (GAP): torch.Size([64, 160, 1, 1])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_81/1341851955.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Train=47.78% | Val=44.66% | ValLoss=1.2667\nEpoch 02 | Train=58.01% | Val=47.34% | ValLoss=1.2137\nEpoch 03 | Train=62.44% | Val=49.61% | ValLoss=1.2396\nEpoch 04 | Train=64.20% | Val=40.49% | ValLoss=1.4741\nEpoch 05 | Train=65.58% | Val=53.37% | ValLoss=1.1571\nEpoch 06 | Train=66.54% | Val=46.51% | ValLoss=1.3653\nEpoch 07 | Train=67.91% | Val=51.24% | ValLoss=1.2138\nEpoch 08 | Train=68.40% | Val=43.80% | ValLoss=1.4297\nEpoch 09 | Train=68.70% | Val=45.32% | ValLoss=1.4631\nEpoch 10 | Train=70.19% | Val=52.17% | ValLoss=1.2869\nEpoch 11 | Train=69.75% | Val=50.58% | ValLoss=1.2950\nEpoch 12 | Train=72.71% | Val=46.15% | ValLoss=1.3904\nEpoch 13 | Train=73.53% | Val=54.28% | ValLoss=1.2070\nEpoch 14 | Train=74.15% | Val=52.30% | ValLoss=1.3943\nEpoch 15 | Train=74.55% | Val=50.42% | ValLoss=1.2844\nEpoch 16 | Train=74.37% | Val=49.08% | ValLoss=1.3586\nEpoch 17 | Train=74.57% | Val=43.69% | ValLoss=1.5455\nEpoch 18 | Train=75.35% | Val=52.88% | ValLoss=1.2485\nEpoch 19 | Train=75.85% | Val=51.81% | ValLoss=1.2708\nEpoch 20 | Train=75.88% | Val=52.14% | ValLoss=1.2713\nEpoch 21 | Train=76.06% | Val=51.38% | ValLoss=1.2625\nEpoch 22 | Train=76.33% | Val=49.82% | ValLoss=1.3348\nEpoch 23 | Train=76.65% | Val=52.72% | ValLoss=1.2574\nEpoch 24 | Train=76.53% | Val=53.15% | ValLoss=1.2361\nEpoch 25 | Train=77.09% | Val=51.58% | ValLoss=1.2759\nEpoch 26 | Train=76.64% | Val=52.88% | ValLoss=1.2572\nEpoch 27 | Train=76.90% | Val=51.09% | ValLoss=1.3027\nEpoch 28 | Train=76.55% | Val=52.68% | ValLoss=1.2730\nEarly stopping (no improvement for 15 epochs).\n\nFinal Test Accuracy: 54.28%  (best seen during training: 54.28%)\n","output_type":"stream"}],"execution_count":25}]}