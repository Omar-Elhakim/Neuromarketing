{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb65171",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-11T14:09:15.570949Z",
     "iopub.status.busy": "2025-12-11T14:09:15.570325Z",
     "iopub.status.idle": "2025-12-11T14:22:56.319665Z",
     "shell.execute_reply": "2025-12-11T14:22:56.318676Z"
    },
    "papermill": {
     "duration": 820.75496,
     "end_time": "2025-12-11T14:22:56.321289",
     "exception": false,
     "start_time": "2025-12-11T14:09:15.566329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m251.4/251.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m231.5/231.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m295.1/295.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m124.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m274.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m750.2/750.2 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Building wheel for torcheeg (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Building wheel for spectrum (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "kaggle-environments 1.18.0 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\r\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "cvxpy 1.6.7 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\r\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "xarray-einstats 0.9.1 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\r\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch_scatter torcheeg torch_geometric -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e139a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T14:22:56.359076Z",
     "iopub.status.busy": "2025-12-11T14:22:56.358451Z",
     "iopub.status.idle": "2025-12-11T14:23:09.100255Z",
     "shell.execute_reply": "2025-12-11T14:23:09.099425Z"
    },
    "papermill": {
     "duration": 12.762832,
     "end_time": "2025-12-11T14:23:09.101758",
     "exception": false,
     "start_time": "2025-12-11T14:22:56.338926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as utils\n",
    "from torch.utils.data import DataLoader, Subset,WeightedRandomSampler\n",
    "from torcheeg.models import CCNN\n",
    "from torcheeg import transforms\n",
    "from torcheeg.transforms import ToGrid\n",
    "from torcheeg.datasets import SEEDIVDataset,SEEDIVFeatureDataset\n",
    "from torcheeg.datasets.constants import SEED_IV_CHANNEL_LOCATION_DICT\n",
    "from torcheeg.transforms import ToG\n",
    "from torcheeg.datasets.constants import SEED_IV_ADJACENCY_MATRIX\n",
    "from torcheeg.models import DGCNN\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "# --- THE MAIN SUBJECT LOOP ---\n",
    "\n",
    "import torch_geometric.loader as geom_loader # Special loader for graphs\n",
    "import copy\n",
    "import scipy.signal as signal\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff77ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T14:23:09.135941Z",
     "iopub.status.busy": "2025-12-11T14:23:09.135471Z",
     "iopub.status.idle": "2025-12-11T14:23:09.139416Z",
     "shell.execute_reply": "2025-12-11T14:23:09.138856Z"
    },
    "papermill": {
     "duration": 0.021942,
     "end_time": "2025-12-11T14:23:09.140440",
     "exception": false,
     "start_time": "2025-12-11T14:23:09.118498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "if os.path.exists('./tmp_out/seed_iv_features'):\n",
    "    shutil.rmtree('./tmp_out/seed_iv_features')\n",
    "    print(\"Old cache deleted. Data will be re-processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cf4a8de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T14:23:09.173979Z",
     "iopub.status.busy": "2025-12-11T14:23:09.173743Z",
     "iopub.status.idle": "2025-12-11T14:23:09.176996Z",
     "shell.execute_reply": "2025-12-11T14:23:09.176445Z"
    },
    "papermill": {
     "duration": 0.021193,
     "end_time": "2025-12-11T14:23:09.177952",
     "exception": false,
     "start_time": "2025-12-11T14:23:09.156759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Setup Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27bbfde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T14:23:09.212028Z",
     "iopub.status.busy": "2025-12-11T14:23:09.211319Z",
     "iopub.status.idle": "2025-12-11T14:26:41.289457Z",
     "shell.execute_reply": "2025-12-11T14:26:41.288868Z"
    },
    "papermill": {
     "duration": 212.096372,
     "end_time": "2025-12-11T14:26:41.290724",
     "exception": false,
     "start_time": "2025-12-11T14:23:09.194352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-11 14:23:09] INFO (torcheeg/MainThread) üîç | Processing EEG data. Processed EEG data has been cached to \u001b[92m./tmp_out/seed_iv_features\u001b[0m.\n",
      "[2025-12-11 14:23:09] INFO (torcheeg/MainThread) ‚è≥ | Monitoring the detailed processing of a record for debugging. The processing of other records will only be reported in percentage to keep it clean.\n",
      "[PROCESS]:   0%|          | 0/45 [00:00<?, ?it/s]\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 0it [00:00, ?it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 1it [00:00,  8.91it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 17it [00:00, 92.54it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 35it [00:00, 129.75it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 54it [00:00, 152.20it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 73it [00:00, 165.07it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 91it [00:00, 169.70it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 109it [00:00, 169.59it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 128it [00:00, 174.05it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 147it [00:00, 178.53it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 167it [00:01, 183.37it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 186it [00:01, 181.69it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 205it [00:01, 182.89it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 224it [00:01, 182.78it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 243it [00:01, 183.66it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 262it [00:01, 177.60it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 281it [00:01, 179.18it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 299it [00:01, 177.01it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 317it [00:01, 177.53it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 335it [00:01, 175.01it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 353it [00:02, 166.33it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 370it [00:02, 160.08it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 387it [00:02, 156.57it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 403it [00:02, 154.80it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 419it [00:02, 155.37it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 438it [00:02, 164.60it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 457it [00:02, 169.94it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 475it [00:02, 171.94it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 494it [00:02, 175.02it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 513it [00:03, 177.58it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 531it [00:03, 175.16it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 550it [00:03, 177.18it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 568it [00:03, 177.70it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 586it [00:03, 176.40it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 605it [00:03, 177.59it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 624it [00:03, 179.81it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 643it [00:03, 181.93it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 663it [00:03, 185.01it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 683it [00:03, 188.56it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 703it [00:04, 190.30it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 723it [00:04, 191.08it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 743it [00:04, 189.04it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 763it [00:04, 190.22it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 783it [00:04, 190.15it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 803it [00:04, 190.00it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 823it [00:04, 187.58it/s]\u001b[A\n",
      "[RECORD /kaggle/input/seed-iv/eeg_feature_smooth/1/4_20151111.mat]: 842it [00:04, 187.74it/s]\u001b[A\n",
      "[PROCESS]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [03:31<00:00,  4.71s/it]\n",
      "[2025-12-11 14:26:41] INFO (torcheeg/MainThread) ‚úÖ | All processed EEG data has been cached to ./tmp_out/seed_iv_features.\n",
      "[2025-12-11 14:26:41] INFO (torcheeg/MainThread) üòä | Please set \u001b[92mio_path\u001b[0m to \u001b[92m./tmp_out/seed_iv_features\u001b[0m for the next run, to directly read from the cache if you wish to skip the data processing step.\n"
     ]
    }
   ],
   "source": [
    "from torcheeg.datasets.constants import SEED_IV_CHANNEL_LOCATION_DICT\n",
    "from torcheeg.transforms import ToGrid\n",
    "\n",
    "dataset = SEEDIVFeatureDataset(\n",
    "    io_path='./tmp_out/seed_iv_features',\n",
    "    root_path='/kaggle/input/seed-iv/eeg_feature_smooth',\n",
    "    feature=['de_LDS'],\n",
    "    num_worker=0,\n",
    "    offline_transform=transforms.Compose([\n",
    "        ToGrid(SEED_IV_CHANNEL_LOCATION_DICT),\n",
    "        transforms.MinMaxNormalize(),\n",
    "        transforms.Lambda(lambda x: torch.tensor(x).float())\n",
    "        \n",
    "    ]),\n",
    "    label_transform=transforms.Compose([\n",
    "        transforms.Select('emotion')\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42af7216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T14:26:41.339876Z",
     "iopub.status.busy": "2025-12-11T14:26:41.339320Z",
     "iopub.status.idle": "2025-12-11T14:26:41.343145Z",
     "shell.execute_reply": "2025-12-11T14:26:41.342578Z"
    },
    "papermill": {
     "duration": 0.029136,
     "end_time": "2025-12-11T14:26:41.344201",
     "exception": false,
     "start_time": "2025-12-11T14:26:41.315065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eec1e1e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T14:26:41.392999Z",
     "iopub.status.busy": "2025-12-11T14:26:41.392787Z",
     "iopub.status.idle": "2025-12-11T14:26:41.401886Z",
     "shell.execute_reply": "2025-12-11T14:26:41.401386Z"
    },
    "papermill": {
     "duration": 0.034301,
     "end_time": "2025-12-11T14:26:41.402875",
     "exception": false,
     "start_time": "2025-12-11T14:26:41.368574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e512a22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T14:26:41.451105Z",
     "iopub.status.busy": "2025-12-11T14:26:41.450886Z",
     "iopub.status.idle": "2025-12-11T14:26:41.456421Z",
     "shell.execute_reply": "2025-12-11T14:26:41.455907Z"
    },
    "papermill": {
     "duration": 0.030847,
     "end_time": "2025-12-11T14:26:41.457404",
     "exception": false,
     "start_time": "2025-12-11T14:26:41.426557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "subjects = sorted(dataset.info['subject_id'].unique()) # Get list of all 15 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c280864c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T14:26:41.505326Z",
     "iopub.status.busy": "2025-12-11T14:26:41.505140Z",
     "iopub.status.idle": "2025-12-11T14:26:41.509825Z",
     "shell.execute_reply": "2025-12-11T14:26:41.509198Z"
    },
    "papermill": {
     "duration": 0.030068,
     "end_time": "2025-12-11T14:26:41.511119",
     "exception": false,
     "start_time": "2025-12-11T14:26:41.481051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = out.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b74e9f44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T14:26:41.558854Z",
     "iopub.status.busy": "2025-12-11T14:26:41.558644Z",
     "iopub.status.idle": "2025-12-11T14:26:41.562877Z",
     "shell.execute_reply": "2025-12-11T14:26:41.562386Z"
    },
    "papermill": {
     "duration": 0.029318,
     "end_time": "2025-12-11T14:26:41.563888",
     "exception": false,
     "start_time": "2025-12-11T14:26:41.534570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0].to(device)\n",
    "            y = batch[1].to(device)\n",
    "\n",
    "            out = model(x)\n",
    "            \n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b547e4cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T14:26:41.611524Z",
     "iopub.status.busy": "2025-12-11T14:26:41.611332Z",
     "iopub.status.idle": "2025-12-11T14:26:41.614470Z",
     "shell.execute_reply": "2025-12-11T14:26:41.613993Z"
    },
    "papermill": {
     "duration": 0.028207,
     "end_time": "2025-12-11T14:26:41.615388",
     "exception": false,
     "start_time": "2025-12-11T14:26:41.587181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bc35a2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T14:26:41.664784Z",
     "iopub.status.busy": "2025-12-11T14:26:41.664578Z",
     "iopub.status.idle": "2025-12-11T14:43:29.808720Z",
     "shell.execute_reply": "2025-12-11T14:43:29.807988Z"
    },
    "papermill": {
     "duration": 1008.170475,
     "end_time": "2025-12-11T14:43:29.809872",
     "exception": false,
     "start_time": "2025-12-11T14:26:41.639397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing SUBJECT: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:1538: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75 | Train Loss: 1.5693 | Val Loss: 1.4481 | Train Acc: 0.263 | Val Acc: 0.125 (*) BEST\n",
      "Epoch 2/75 | Train Loss: 1.3871 | Val Loss: 1.3785 | Train Acc: 0.272 | Val Acc: 0.412 (*) BEST\n",
      "Epoch 4/75 | Train Loss: 1.3814 | Val Loss: 1.3920 | Train Acc: 0.287 | Val Acc: 0.469 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.3784 | Val Loss: 1.3511 | Train Acc: 0.292 | Val Acc: 0.382 \n",
      "Epoch 10/75 | Train Loss: 1.3798 | Val Loss: 1.4115 | Train Acc: 0.285 | Val Acc: 0.125 \n",
      "Epoch 15/75 | Train Loss: 1.3690 | Val Loss: 1.4045 | Train Acc: 0.321 | Val Acc: 0.241 \n",
      "Epoch 20/75 | Train Loss: 1.3487 | Val Loss: 1.3827 | Train Acc: 0.395 | Val Acc: 0.206 \n",
      "Epoch 25/75 | Train Loss: 1.3355 | Val Loss: 1.3903 | Train Acc: 0.430 | Val Acc: 0.312 \n",
      "Epoch 30/75 | Train Loss: 1.3259 | Val Loss: 1.3745 | Train Acc: 0.438 | Val Acc: 0.333 \n",
      "Epoch 35/75 | Train Loss: 1.3217 | Val Loss: 1.3746 | Train Acc: 0.478 | Val Acc: 0.307 \n",
      "Epoch 40/75 | Train Loss: 1.3151 | Val Loss: 1.3925 | Train Acc: 0.524 | Val Acc: 0.312 \n",
      "Epoch 45/75 | Train Loss: 1.3168 | Val Loss: 1.3827 | Train Acc: 0.525 | Val Acc: 0.307 \n",
      "Epoch 50/75 | Train Loss: 1.3117 | Val Loss: 1.3811 | Train Acc: 0.536 | Val Acc: 0.309 \n",
      "Epoch 55/75 | Train Loss: 1.3093 | Val Loss: 1.3829 | Train Acc: 0.542 | Val Acc: 0.307 \n",
      "Epoch 60/75 | Train Loss: 1.3141 | Val Loss: 1.3851 | Train Acc: 0.520 | Val Acc: 0.241 \n",
      "Epoch 65/75 | Train Loss: 1.3089 | Val Loss: 1.3848 | Train Acc: 0.534 | Val Acc: 0.243 \n",
      "Epoch 70/75 | Train Loss: 1.3122 | Val Loss: 1.3838 | Train Acc: 0.529 | Val Acc: 0.241 \n",
      "Epoch 75/75 | Train Loss: 1.3082 | Val Loss: 1.3838 | Train Acc: 0.544 | Val Acc: 0.241 \n",
      "Subject 1 FINAL BEST ACC: 46.88%\n",
      "\n",
      "Processing SUBJECT: 2\n",
      "Epoch 1/75 | Train Loss: 1.5951 | Val Loss: 1.4417 | Train Acc: 0.283 | Val Acc: 0.173 (*) BEST\n",
      "Epoch 2/75 | Train Loss: 1.3345 | Val Loss: 1.3773 | Train Acc: 0.385 | Val Acc: 0.237 (*) BEST\n",
      "Epoch 3/75 | Train Loss: 1.2312 | Val Loss: 1.3057 | Train Acc: 0.509 | Val Acc: 0.438 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.0485 | Val Loss: 1.2181 | Train Acc: 0.626 | Val Acc: 0.487 (*) BEST\n",
      "Epoch 7/75 | Train Loss: 1.0307 | Val Loss: 1.2862 | Train Acc: 0.605 | Val Acc: 0.596 (*) BEST\n",
      "Epoch 10/75 | Train Loss: 0.8329 | Val Loss: 1.3751 | Train Acc: 0.745 | Val Acc: 0.596 \n",
      "Epoch 13/75 | Train Loss: 0.7301 | Val Loss: 1.1546 | Train Acc: 0.833 | Val Acc: 0.669 (*) BEST\n",
      "Epoch 15/75 | Train Loss: 0.6810 | Val Loss: 1.0459 | Train Acc: 0.861 | Val Acc: 0.618 \n",
      "Epoch 20/75 | Train Loss: 0.5974 | Val Loss: 1.0457 | Train Acc: 0.902 | Val Acc: 0.557 \n",
      "Epoch 21/75 | Train Loss: 0.5521 | Val Loss: 1.0538 | Train Acc: 0.919 | Val Acc: 0.686 (*) BEST\n",
      "Epoch 25/75 | Train Loss: 0.5022 | Val Loss: 1.0929 | Train Acc: 0.955 | Val Acc: 0.680 \n",
      "Epoch 28/75 | Train Loss: 0.4921 | Val Loss: 1.1062 | Train Acc: 0.952 | Val Acc: 0.739 (*) BEST\n",
      "Epoch 29/75 | Train Loss: 0.4800 | Val Loss: 1.0106 | Train Acc: 0.959 | Val Acc: 0.741 (*) BEST\n",
      "Epoch 30/75 | Train Loss: 0.4655 | Val Loss: 1.1341 | Train Acc: 0.971 | Val Acc: 0.772 (*) BEST\n",
      "Epoch 35/75 | Train Loss: 0.4525 | Val Loss: 1.0662 | Train Acc: 0.977 | Val Acc: 0.649 \n",
      "Epoch 40/75 | Train Loss: 0.4331 | Val Loss: 1.0213 | Train Acc: 0.989 | Val Acc: 0.695 \n",
      "Epoch 45/75 | Train Loss: 0.4311 | Val Loss: 1.0686 | Train Acc: 0.993 | Val Acc: 0.752 \n",
      "Epoch 50/75 | Train Loss: 0.4233 | Val Loss: 1.0302 | Train Acc: 0.995 | Val Acc: 0.680 \n",
      "Epoch 55/75 | Train Loss: 0.4214 | Val Loss: 1.0359 | Train Acc: 0.994 | Val Acc: 0.676 \n",
      "Epoch 60/75 | Train Loss: 0.4213 | Val Loss: 1.0421 | Train Acc: 0.994 | Val Acc: 0.678 \n",
      "Epoch 65/75 | Train Loss: 0.4184 | Val Loss: 1.0351 | Train Acc: 0.996 | Val Acc: 0.669 \n",
      "Epoch 70/75 | Train Loss: 0.4175 | Val Loss: 1.0388 | Train Acc: 0.995 | Val Acc: 0.676 \n",
      "Epoch 75/75 | Train Loss: 0.4184 | Val Loss: 1.0408 | Train Acc: 0.997 | Val Acc: 0.676 \n",
      "Subject 2 FINAL BEST ACC: 77.21%\n",
      "\n",
      "Processing SUBJECT: 3\n",
      "Epoch 1/75 | Train Loss: 1.7700 | Val Loss: 1.4129 | Train Acc: 0.258 | Val Acc: 0.320 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.3698 | Val Loss: 1.4532 | Train Acc: 0.310 | Val Acc: 0.101 \n",
      "Epoch 10/75 | Train Loss: 1.3632 | Val Loss: 1.4571 | Train Acc: 0.324 | Val Acc: 0.101 \n",
      "Epoch 15/75 | Train Loss: 1.3496 | Val Loss: 1.4231 | Train Acc: 0.338 | Val Acc: 0.173 \n",
      "Epoch 16/75 | Train Loss: 1.3512 | Val Loss: 1.4213 | Train Acc: 0.345 | Val Acc: 0.494 (*) BEST\n",
      "Epoch 20/75 | Train Loss: 1.3416 | Val Loss: 1.4365 | Train Acc: 0.367 | Val Acc: 0.184 \n",
      "Epoch 25/75 | Train Loss: 1.3329 | Val Loss: 1.4619 | Train Acc: 0.366 | Val Acc: 0.123 \n",
      "Epoch 30/75 | Train Loss: 1.3141 | Val Loss: 1.4392 | Train Acc: 0.413 | Val Acc: 0.213 \n",
      "Epoch 35/75 | Train Loss: 1.3134 | Val Loss: 1.4303 | Train Acc: 0.381 | Val Acc: 0.213 \n",
      "Epoch 40/75 | Train Loss: 1.3086 | Val Loss: 1.4278 | Train Acc: 0.402 | Val Acc: 0.285 \n",
      "Epoch 45/75 | Train Loss: 1.3055 | Val Loss: 1.4414 | Train Acc: 0.419 | Val Acc: 0.213 \n",
      "Epoch 50/75 | Train Loss: 1.2992 | Val Loss: 1.4417 | Train Acc: 0.434 | Val Acc: 0.213 \n",
      "Epoch 55/75 | Train Loss: 1.3010 | Val Loss: 1.4343 | Train Acc: 0.435 | Val Acc: 0.213 \n",
      "Epoch 60/75 | Train Loss: 1.2966 | Val Loss: 1.4323 | Train Acc: 0.452 | Val Acc: 0.213 \n",
      "Epoch 65/75 | Train Loss: 1.3081 | Val Loss: 1.4301 | Train Acc: 0.423 | Val Acc: 0.213 \n",
      "Epoch 70/75 | Train Loss: 1.3052 | Val Loss: 1.4281 | Train Acc: 0.432 | Val Acc: 0.213 \n",
      "Epoch 75/75 | Train Loss: 1.3026 | Val Loss: 1.4289 | Train Acc: 0.457 | Val Acc: 0.213 \n",
      "Subject 3 FINAL BEST ACC: 49.45%\n",
      "\n",
      "Processing SUBJECT: 4\n",
      "Epoch 1/75 | Train Loss: 1.7623 | Val Loss: 1.3342 | Train Acc: 0.267 | Val Acc: 0.382 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.3241 | Val Loss: 1.3327 | Train Acc: 0.411 | Val Acc: 0.351 \n",
      "Epoch 6/75 | Train Loss: 1.2769 | Val Loss: 1.2712 | Train Acc: 0.445 | Val Acc: 0.465 (*) BEST\n",
      "Epoch 9/75 | Train Loss: 1.2486 | Val Loss: 1.3467 | Train Acc: 0.466 | Val Acc: 0.494 (*) BEST\n",
      "Epoch 10/75 | Train Loss: 1.2300 | Val Loss: 1.2901 | Train Acc: 0.468 | Val Acc: 0.419 \n",
      "Epoch 13/75 | Train Loss: 1.1730 | Val Loss: 1.2759 | Train Acc: 0.547 | Val Acc: 0.520 (*) BEST\n",
      "Epoch 15/75 | Train Loss: 1.1410 | Val Loss: 1.2338 | Train Acc: 0.543 | Val Acc: 0.423 \n",
      "Epoch 17/75 | Train Loss: 1.1035 | Val Loss: 1.2674 | Train Acc: 0.593 | Val Acc: 0.555 (*) BEST\n",
      "Epoch 20/75 | Train Loss: 1.0377 | Val Loss: 1.0893 | Train Acc: 0.611 | Val Acc: 0.478 \n",
      "Epoch 21/75 | Train Loss: 1.0170 | Val Loss: 1.0644 | Train Acc: 0.637 | Val Acc: 0.645 (*) BEST\n",
      "Epoch 25/75 | Train Loss: 0.8860 | Val Loss: 1.0261 | Train Acc: 0.708 | Val Acc: 0.741 (*) BEST\n",
      "Epoch 30/75 | Train Loss: 0.7151 | Val Loss: 0.9453 | Train Acc: 0.847 | Val Acc: 0.763 (*) BEST\n",
      "Epoch 35/75 | Train Loss: 0.6054 | Val Loss: 0.8295 | Train Acc: 0.899 | Val Acc: 0.763 \n",
      "Epoch 40/75 | Train Loss: 0.5748 | Val Loss: 0.8149 | Train Acc: 0.923 | Val Acc: 0.590 \n",
      "Epoch 45/75 | Train Loss: 0.4971 | Val Loss: 0.7397 | Train Acc: 0.971 | Val Acc: 0.763 \n",
      "Epoch 50/75 | Train Loss: 0.4839 | Val Loss: 0.9023 | Train Acc: 0.976 | Val Acc: 0.763 \n",
      "Epoch 55/75 | Train Loss: 0.4527 | Val Loss: 0.7031 | Train Acc: 0.994 | Val Acc: 0.763 \n",
      "Epoch 60/75 | Train Loss: 0.4423 | Val Loss: 0.7613 | Train Acc: 0.994 | Val Acc: 0.763 \n",
      "Epoch 65/75 | Train Loss: 0.4352 | Val Loss: 0.7389 | Train Acc: 0.995 | Val Acc: 0.763 \n",
      "Epoch 70/75 | Train Loss: 0.4273 | Val Loss: 0.7375 | Train Acc: 0.995 | Val Acc: 0.763 \n",
      "Epoch 75/75 | Train Loss: 0.4257 | Val Loss: 0.7849 | Train Acc: 0.995 | Val Acc: 0.763 \n",
      "Subject 4 FINAL BEST ACC: 76.29%\n",
      "\n",
      "Processing SUBJECT: 5\n",
      "Epoch 1/75 | Train Loss: 1.7839 | Val Loss: 1.4114 | Train Acc: 0.245 | Val Acc: 0.173 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.3726 | Val Loss: 1.4344 | Train Acc: 0.306 | Val Acc: 0.125 \n",
      "Epoch 6/75 | Train Loss: 1.3775 | Val Loss: 1.3854 | Train Acc: 0.294 | Val Acc: 0.331 (*) BEST\n",
      "Epoch 7/75 | Train Loss: 1.3711 | Val Loss: 1.3745 | Train Acc: 0.303 | Val Acc: 0.358 (*) BEST\n",
      "Epoch 10/75 | Train Loss: 1.3519 | Val Loss: 1.4720 | Train Acc: 0.336 | Val Acc: 0.101 \n",
      "Epoch 12/75 | Train Loss: 1.3377 | Val Loss: 1.4121 | Train Acc: 0.361 | Val Acc: 0.381 (*) BEST\n",
      "Epoch 15/75 | Train Loss: 1.3289 | Val Loss: 1.4667 | Train Acc: 0.375 | Val Acc: 0.276 \n",
      "Epoch 20/75 | Train Loss: 1.2751 | Val Loss: 1.3645 | Train Acc: 0.459 | Val Acc: 0.267 \n",
      "Epoch 25/75 | Train Loss: 1.2462 | Val Loss: 1.3660 | Train Acc: 0.490 | Val Acc: 0.318 \n",
      "Epoch 30/75 | Train Loss: 1.2143 | Val Loss: 1.3805 | Train Acc: 0.520 | Val Acc: 0.318 \n",
      "Epoch 35/75 | Train Loss: 1.1858 | Val Loss: 1.3623 | Train Acc: 0.532 | Val Acc: 0.318 \n",
      "Epoch 39/75 | Train Loss: 1.1588 | Val Loss: 1.3294 | Train Acc: 0.558 | Val Acc: 0.412 (*) BEST\n",
      "Epoch 40/75 | Train Loss: 1.1337 | Val Loss: 1.3390 | Train Acc: 0.619 | Val Acc: 0.373 \n",
      "Epoch 45/75 | Train Loss: 1.1183 | Val Loss: 1.3396 | Train Acc: 0.619 | Val Acc: 0.364 \n",
      "Epoch 50/75 | Train Loss: 1.0903 | Val Loss: 1.3413 | Train Acc: 0.648 | Val Acc: 0.353 \n",
      "Epoch 55/75 | Train Loss: 1.0587 | Val Loss: 1.3328 | Train Acc: 0.667 | Val Acc: 0.314 \n",
      "Epoch 60/75 | Train Loss: 1.0309 | Val Loss: 1.3242 | Train Acc: 0.706 | Val Acc: 0.353 \n",
      "Epoch 65/75 | Train Loss: 1.0086 | Val Loss: 1.2965 | Train Acc: 0.710 | Val Acc: 0.412 \n",
      "Epoch 70/75 | Train Loss: 1.0041 | Val Loss: 1.3076 | Train Acc: 0.697 | Val Acc: 0.392 \n",
      "Epoch 75/75 | Train Loss: 0.9930 | Val Loss: 1.2787 | Train Acc: 0.709 | Val Acc: 0.412 \n",
      "Subject 5 FINAL BEST ACC: 41.18%\n",
      "\n",
      "Processing SUBJECT: 6\n",
      "Epoch 1/75 | Train Loss: 1.6701 | Val Loss: 1.3701 | Train Acc: 0.270 | Val Acc: 0.320 (*) BEST\n",
      "Epoch 3/75 | Train Loss: 1.3614 | Val Loss: 1.3715 | Train Acc: 0.350 | Val Acc: 0.382 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.3413 | Val Loss: 1.4460 | Train Acc: 0.370 | Val Acc: 0.202 \n",
      "Epoch 6/75 | Train Loss: 1.3355 | Val Loss: 1.3519 | Train Acc: 0.411 | Val Acc: 0.392 (*) BEST\n",
      "Epoch 10/75 | Train Loss: 1.2223 | Val Loss: 1.2726 | Train Acc: 0.459 | Val Acc: 0.555 (*) BEST\n",
      "Epoch 11/75 | Train Loss: 1.1431 | Val Loss: 1.2054 | Train Acc: 0.563 | Val Acc: 0.557 (*) BEST\n",
      "Epoch 12/75 | Train Loss: 1.1040 | Val Loss: 1.1806 | Train Acc: 0.595 | Val Acc: 0.608 (*) BEST\n",
      "Epoch 15/75 | Train Loss: 0.9298 | Val Loss: 1.0887 | Train Acc: 0.711 | Val Acc: 0.518 \n",
      "Epoch 16/75 | Train Loss: 0.9377 | Val Loss: 1.0818 | Train Acc: 0.672 | Val Acc: 0.665 (*) BEST\n",
      "Epoch 18/75 | Train Loss: 0.8268 | Val Loss: 1.0286 | Train Acc: 0.761 | Val Acc: 0.763 (*) BEST\n",
      "Epoch 20/75 | Train Loss: 0.7718 | Val Loss: 1.1687 | Train Acc: 0.791 | Val Acc: 0.513 \n",
      "Epoch 23/75 | Train Loss: 0.7174 | Val Loss: 0.8703 | Train Acc: 0.824 | Val Acc: 0.785 (*) BEST\n",
      "Epoch 25/75 | Train Loss: 0.7640 | Val Loss: 1.0231 | Train Acc: 0.772 | Val Acc: 0.562 \n",
      "Epoch 30/75 | Train Loss: 0.6852 | Val Loss: 1.2653 | Train Acc: 0.837 | Val Acc: 0.450 \n",
      "Epoch 35/75 | Train Loss: 0.5799 | Val Loss: 0.9103 | Train Acc: 0.908 | Val Acc: 0.616 \n",
      "Epoch 40/75 | Train Loss: 0.5473 | Val Loss: 0.9557 | Train Acc: 0.942 | Val Acc: 0.603 \n",
      "Epoch 45/75 | Train Loss: 0.5201 | Val Loss: 0.9408 | Train Acc: 0.962 | Val Acc: 0.539 \n",
      "Epoch 50/75 | Train Loss: 0.5035 | Val Loss: 0.9525 | Train Acc: 0.967 | Val Acc: 0.539 \n",
      "Epoch 55/75 | Train Loss: 0.4962 | Val Loss: 0.9715 | Train Acc: 0.970 | Val Acc: 0.557 \n",
      "Epoch 60/75 | Train Loss: 0.4943 | Val Loss: 0.9401 | Train Acc: 0.973 | Val Acc: 0.616 \n",
      "Epoch 65/75 | Train Loss: 0.4911 | Val Loss: 0.9380 | Train Acc: 0.964 | Val Acc: 0.616 \n",
      "Epoch 70/75 | Train Loss: 0.4893 | Val Loss: 0.9454 | Train Acc: 0.971 | Val Acc: 0.599 \n",
      "Epoch 75/75 | Train Loss: 0.4875 | Val Loss: 0.9687 | Train Acc: 0.972 | Val Acc: 0.539 \n",
      "Subject 6 FINAL BEST ACC: 78.49%\n",
      "\n",
      "Processing SUBJECT: 7\n",
      "Epoch 1/75 | Train Loss: 1.7285 | Val Loss: 1.3581 | Train Acc: 0.259 | Val Acc: 0.382 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.3464 | Val Loss: 1.3718 | Train Acc: 0.377 | Val Acc: 0.173 \n",
      "Epoch 9/75 | Train Loss: 1.2745 | Val Loss: 1.3539 | Train Acc: 0.411 | Val Acc: 0.425 (*) BEST\n",
      "Epoch 10/75 | Train Loss: 1.2096 | Val Loss: 1.3554 | Train Acc: 0.542 | Val Acc: 0.425 \n",
      "Epoch 12/75 | Train Loss: 1.1892 | Val Loss: 1.2870 | Train Acc: 0.539 | Val Acc: 0.471 (*) BEST\n",
      "Epoch 13/75 | Train Loss: 1.1590 | Val Loss: 1.2218 | Train Acc: 0.571 | Val Acc: 0.496 (*) BEST\n",
      "Epoch 15/75 | Train Loss: 1.1378 | Val Loss: 1.2053 | Train Acc: 0.611 | Val Acc: 0.631 (*) BEST\n",
      "Epoch 16/75 | Train Loss: 1.1263 | Val Loss: 1.1602 | Train Acc: 0.576 | Val Acc: 0.634 (*) BEST\n",
      "Epoch 20/75 | Train Loss: 1.0592 | Val Loss: 1.2235 | Train Acc: 0.606 | Val Acc: 0.386 \n",
      "Epoch 25/75 | Train Loss: 0.9760 | Val Loss: 1.0397 | Train Acc: 0.714 | Val Acc: 0.539 \n",
      "Epoch 26/75 | Train Loss: 0.9338 | Val Loss: 1.0125 | Train Acc: 0.738 | Val Acc: 0.702 (*) BEST\n",
      "Epoch 30/75 | Train Loss: 0.8661 | Val Loss: 1.0191 | Train Acc: 0.793 | Val Acc: 0.520 \n",
      "Epoch 35/75 | Train Loss: 0.8198 | Val Loss: 0.9523 | Train Acc: 0.811 | Val Acc: 0.579 \n",
      "Epoch 40/75 | Train Loss: 0.7456 | Val Loss: 0.8813 | Train Acc: 0.841 | Val Acc: 0.779 (*) BEST\n",
      "Epoch 45/75 | Train Loss: 0.7164 | Val Loss: 0.9390 | Train Acc: 0.840 | Val Acc: 0.540 \n",
      "Epoch 50/75 | Train Loss: 0.6681 | Val Loss: 0.8372 | Train Acc: 0.854 | Val Acc: 0.608 \n",
      "Epoch 51/75 | Train Loss: 0.6418 | Val Loss: 0.8201 | Train Acc: 0.885 | Val Acc: 0.800 (*) BEST\n",
      "Epoch 55/75 | Train Loss: 0.6687 | Val Loss: 0.8262 | Train Acc: 0.860 | Val Acc: 0.730 \n",
      "Epoch 56/75 | Train Loss: 0.6336 | Val Loss: 0.7697 | Train Acc: 0.877 | Val Acc: 0.805 (*) BEST\n",
      "Epoch 57/75 | Train Loss: 0.6346 | Val Loss: 0.7541 | Train Acc: 0.880 | Val Acc: 0.818 (*) BEST\n",
      "Epoch 60/75 | Train Loss: 0.6435 | Val Loss: 0.9481 | Train Acc: 0.873 | Val Acc: 0.590 \n",
      "Epoch 65/75 | Train Loss: 0.5818 | Val Loss: 0.8142 | Train Acc: 0.916 | Val Acc: 0.654 \n",
      "Epoch 67/75 | Train Loss: 0.5794 | Val Loss: 0.7607 | Train Acc: 0.924 | Val Acc: 0.829 (*) BEST\n",
      "Epoch 68/75 | Train Loss: 0.5675 | Val Loss: 0.7478 | Train Acc: 0.921 | Val Acc: 0.842 (*) BEST\n",
      "Epoch 70/75 | Train Loss: 0.5620 | Val Loss: 0.7932 | Train Acc: 0.929 | Val Acc: 0.669 \n",
      "Epoch 75/75 | Train Loss: 0.5590 | Val Loss: 0.7263 | Train Acc: 0.937 | Val Acc: 0.822 \n",
      "Subject 7 FINAL BEST ACC: 84.19%\n",
      "\n",
      "Processing SUBJECT: 8\n",
      "Epoch 1/75 | Train Loss: 1.6547 | Val Loss: 1.3857 | Train Acc: 0.259 | Val Acc: 0.320 (*) BEST\n",
      "Epoch 3/75 | Train Loss: 1.3796 | Val Loss: 1.3736 | Train Acc: 0.276 | Val Acc: 0.382 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.3536 | Val Loss: 1.3923 | Train Acc: 0.361 | Val Acc: 0.268 \n",
      "Epoch 10/75 | Train Loss: 1.2454 | Val Loss: 1.4248 | Train Acc: 0.467 | Val Acc: 0.182 \n",
      "Epoch 15/75 | Train Loss: 1.1480 | Val Loss: 1.4136 | Train Acc: 0.537 | Val Acc: 0.173 \n",
      "Epoch 20/75 | Train Loss: 1.0766 | Val Loss: 1.3845 | Train Acc: 0.551 | Val Acc: 0.210 \n",
      "Epoch 25/75 | Train Loss: 1.0226 | Val Loss: 1.3459 | Train Acc: 0.634 | Val Acc: 0.278 \n",
      "Epoch 27/75 | Train Loss: 1.0079 | Val Loss: 1.3661 | Train Acc: 0.615 | Val Acc: 0.406 (*) BEST\n",
      "Epoch 30/75 | Train Loss: 0.9910 | Val Loss: 1.4362 | Train Acc: 0.660 | Val Acc: 0.300 \n",
      "Epoch 35/75 | Train Loss: 0.9695 | Val Loss: 1.4138 | Train Acc: 0.654 | Val Acc: 0.193 \n",
      "Epoch 40/75 | Train Loss: 0.9483 | Val Loss: 1.3527 | Train Acc: 0.677 | Val Acc: 0.224 \n",
      "Epoch 43/75 | Train Loss: 0.9473 | Val Loss: 1.3589 | Train Acc: 0.673 | Val Acc: 0.430 (*) BEST\n",
      "Epoch 45/75 | Train Loss: 0.9541 | Val Loss: 1.3703 | Train Acc: 0.656 | Val Acc: 0.358 \n",
      "Epoch 50/75 | Train Loss: 0.9594 | Val Loss: 1.3659 | Train Acc: 0.683 | Val Acc: 0.325 \n",
      "Epoch 55/75 | Train Loss: 0.9523 | Val Loss: 1.3638 | Train Acc: 0.663 | Val Acc: 0.360 \n",
      "Epoch 60/75 | Train Loss: 0.9448 | Val Loss: 1.3667 | Train Acc: 0.691 | Val Acc: 0.381 \n",
      "Epoch 65/75 | Train Loss: 0.9409 | Val Loss: 1.3619 | Train Acc: 0.684 | Val Acc: 0.358 \n",
      "Epoch 70/75 | Train Loss: 0.9433 | Val Loss: 1.3677 | Train Acc: 0.689 | Val Acc: 0.369 \n",
      "Epoch 75/75 | Train Loss: 0.9336 | Val Loss: 1.3700 | Train Acc: 0.700 | Val Acc: 0.377 \n",
      "Subject 8 FINAL BEST ACC: 43.01%\n",
      "\n",
      "Processing SUBJECT: 9\n",
      "Epoch 1/75 | Train Loss: 1.6619 | Val Loss: 1.3886 | Train Acc: 0.263 | Val Acc: 0.173 (*) BEST\n",
      "Epoch 4/75 | Train Loss: 1.3496 | Val Loss: 1.4393 | Train Acc: 0.357 | Val Acc: 0.178 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.3572 | Val Loss: 1.3722 | Train Acc: 0.330 | Val Acc: 0.184 (*) BEST\n",
      "Epoch 6/75 | Train Loss: 1.3479 | Val Loss: 1.5021 | Train Acc: 0.340 | Val Acc: 0.226 (*) BEST\n",
      "Epoch 7/75 | Train Loss: 1.3361 | Val Loss: 1.3710 | Train Acc: 0.379 | Val Acc: 0.246 (*) BEST\n",
      "Epoch 8/75 | Train Loss: 1.3296 | Val Loss: 1.3430 | Train Acc: 0.355 | Val Acc: 0.263 (*) BEST\n",
      "Epoch 10/75 | Train Loss: 1.2839 | Val Loss: 1.3441 | Train Acc: 0.395 | Val Acc: 0.149 \n",
      "Epoch 11/75 | Train Loss: 1.2531 | Val Loss: 1.2891 | Train Acc: 0.406 | Val Acc: 0.388 (*) BEST\n",
      "Epoch 14/75 | Train Loss: 1.1905 | Val Loss: 1.2402 | Train Acc: 0.464 | Val Acc: 0.406 (*) BEST\n",
      "Epoch 15/75 | Train Loss: 1.1566 | Val Loss: 1.2377 | Train Acc: 0.518 | Val Acc: 0.312 \n",
      "Epoch 16/75 | Train Loss: 1.1201 | Val Loss: 1.1162 | Train Acc: 0.520 | Val Acc: 0.522 (*) BEST\n",
      "Epoch 17/75 | Train Loss: 1.0883 | Val Loss: 1.1558 | Train Acc: 0.574 | Val Acc: 0.546 (*) BEST\n",
      "Epoch 18/75 | Train Loss: 1.0517 | Val Loss: 1.2051 | Train Acc: 0.585 | Val Acc: 0.551 (*) BEST\n",
      "Epoch 19/75 | Train Loss: 1.0249 | Val Loss: 1.0246 | Train Acc: 0.608 | Val Acc: 0.590 (*) BEST\n",
      "Epoch 20/75 | Train Loss: 0.9944 | Val Loss: 0.9792 | Train Acc: 0.622 | Val Acc: 0.653 (*) BEST\n",
      "Epoch 21/75 | Train Loss: 0.9734 | Val Loss: 1.0110 | Train Acc: 0.646 | Val Acc: 0.700 (*) BEST\n",
      "Epoch 23/75 | Train Loss: 0.8784 | Val Loss: 0.9551 | Train Acc: 0.725 | Val Acc: 0.811 (*) BEST\n",
      "Epoch 24/75 | Train Loss: 0.7924 | Val Loss: 0.9258 | Train Acc: 0.812 | Val Acc: 0.825 (*) BEST\n",
      "Epoch 25/75 | Train Loss: 0.7628 | Val Loss: 0.9151 | Train Acc: 0.831 | Val Acc: 0.710 \n",
      "Epoch 30/75 | Train Loss: 0.6229 | Val Loss: 0.8396 | Train Acc: 0.892 | Val Acc: 0.689 \n",
      "Epoch 35/75 | Train Loss: 0.5543 | Val Loss: 0.9783 | Train Acc: 0.930 | Val Acc: 0.682 \n",
      "Epoch 40/75 | Train Loss: 0.4691 | Val Loss: 0.8821 | Train Acc: 0.973 | Val Acc: 0.612 \n",
      "Epoch 45/75 | Train Loss: 0.4404 | Val Loss: 0.8792 | Train Acc: 0.992 | Val Acc: 0.695 \n",
      "Epoch 50/75 | Train Loss: 0.4407 | Val Loss: 0.8893 | Train Acc: 0.992 | Val Acc: 0.695 \n",
      "Epoch 55/75 | Train Loss: 0.4240 | Val Loss: 0.8764 | Train Acc: 0.999 | Val Acc: 0.695 \n",
      "Epoch 60/75 | Train Loss: 0.4239 | Val Loss: 0.8884 | Train Acc: 1.000 | Val Acc: 0.695 \n",
      "Epoch 65/75 | Train Loss: 0.4222 | Val Loss: 0.8842 | Train Acc: 0.999 | Val Acc: 0.695 \n",
      "Epoch 70/75 | Train Loss: 0.4163 | Val Loss: 0.8827 | Train Acc: 1.000 | Val Acc: 0.695 \n",
      "Epoch 75/75 | Train Loss: 0.4175 | Val Loss: 0.8897 | Train Acc: 1.000 | Val Acc: 0.695 \n",
      "Subject 9 FINAL BEST ACC: 82.54%\n",
      "\n",
      "Processing SUBJECT: 10\n",
      "Epoch 1/75 | Train Loss: 1.5760 | Val Loss: 1.3542 | Train Acc: 0.270 | Val Acc: 0.445 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.3589 | Val Loss: 1.3747 | Train Acc: 0.332 | Val Acc: 0.441 \n",
      "Epoch 10/75 | Train Loss: 1.2659 | Val Loss: 1.4834 | Train Acc: 0.416 | Val Acc: 0.160 \n",
      "Epoch 13/75 | Train Loss: 1.2140 | Val Loss: 1.1726 | Train Acc: 0.472 | Val Acc: 0.518 (*) BEST\n",
      "Epoch 15/75 | Train Loss: 1.1557 | Val Loss: 1.1874 | Train Acc: 0.511 | Val Acc: 0.585 (*) BEST\n",
      "Epoch 18/75 | Train Loss: 1.0995 | Val Loss: 1.0936 | Train Acc: 0.577 | Val Acc: 0.629 (*) BEST\n",
      "Epoch 20/75 | Train Loss: 1.0374 | Val Loss: 1.1099 | Train Acc: 0.654 | Val Acc: 0.673 (*) BEST\n",
      "Epoch 23/75 | Train Loss: 0.9994 | Val Loss: 1.1205 | Train Acc: 0.632 | Val Acc: 0.706 (*) BEST\n",
      "Epoch 25/75 | Train Loss: 0.9307 | Val Loss: 1.0186 | Train Acc: 0.720 | Val Acc: 0.765 (*) BEST\n",
      "Epoch 30/75 | Train Loss: 0.7785 | Val Loss: 0.9232 | Train Acc: 0.814 | Val Acc: 0.614 \n",
      "Epoch 35/75 | Train Loss: 0.7146 | Val Loss: 0.9321 | Train Acc: 0.857 | Val Acc: 0.629 \n",
      "Epoch 38/75 | Train Loss: 0.6816 | Val Loss: 0.8416 | Train Acc: 0.853 | Val Acc: 0.803 (*) BEST\n",
      "Epoch 40/75 | Train Loss: 0.6376 | Val Loss: 0.8589 | Train Acc: 0.903 | Val Acc: 0.660 \n",
      "Epoch 45/75 | Train Loss: 0.5694 | Val Loss: 0.7978 | Train Acc: 0.926 | Val Acc: 0.664 \n",
      "Epoch 50/75 | Train Loss: 0.5389 | Val Loss: 0.8737 | Train Acc: 0.953 | Val Acc: 0.697 \n",
      "Epoch 55/75 | Train Loss: 0.5105 | Val Loss: 0.7947 | Train Acc: 0.959 | Val Acc: 0.700 \n",
      "Epoch 60/75 | Train Loss: 0.5014 | Val Loss: 0.8460 | Train Acc: 0.960 | Val Acc: 0.676 \n",
      "Epoch 65/75 | Train Loss: 0.4837 | Val Loss: 0.8133 | Train Acc: 0.961 | Val Acc: 0.676 \n",
      "Epoch 70/75 | Train Loss: 0.5094 | Val Loss: 0.8531 | Train Acc: 0.958 | Val Acc: 0.675 \n",
      "Epoch 75/75 | Train Loss: 0.4837 | Val Loss: 0.8287 | Train Acc: 0.967 | Val Acc: 0.676 \n",
      "Subject 10 FINAL BEST ACC: 80.33%\n",
      "\n",
      "Processing SUBJECT: 11\n",
      "Epoch 1/75 | Train Loss: 1.7066 | Val Loss: 1.3823 | Train Acc: 0.267 | Val Acc: 0.312 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.3638 | Val Loss: 1.4699 | Train Acc: 0.322 | Val Acc: 0.101 \n",
      "Epoch 9/75 | Train Loss: 1.3577 | Val Loss: 1.4173 | Train Acc: 0.319 | Val Acc: 0.353 (*) BEST\n",
      "Epoch 10/75 | Train Loss: 1.3505 | Val Loss: 1.4246 | Train Acc: 0.352 | Val Acc: 0.283 \n",
      "Epoch 14/75 | Train Loss: 1.3445 | Val Loss: 1.4048 | Train Acc: 0.361 | Val Acc: 0.415 (*) BEST\n",
      "Epoch 15/75 | Train Loss: 1.3284 | Val Loss: 1.4399 | Train Acc: 0.388 | Val Acc: 0.173 \n",
      "Epoch 20/75 | Train Loss: 1.3174 | Val Loss: 1.4300 | Train Acc: 0.423 | Val Acc: 0.369 \n",
      "Epoch 25/75 | Train Loss: 1.3129 | Val Loss: 1.4230 | Train Acc: 0.408 | Val Acc: 0.318 \n",
      "Epoch 26/75 | Train Loss: 1.3143 | Val Loss: 1.4117 | Train Acc: 0.423 | Val Acc: 0.419 (*) BEST\n",
      "Epoch 30/75 | Train Loss: 1.3059 | Val Loss: 1.4361 | Train Acc: 0.459 | Val Acc: 0.213 \n",
      "Epoch 35/75 | Train Loss: 1.2976 | Val Loss: 1.4316 | Train Acc: 0.495 | Val Acc: 0.285 \n",
      "Epoch 40/75 | Train Loss: 1.2986 | Val Loss: 1.4154 | Train Acc: 0.480 | Val Acc: 0.353 \n",
      "Epoch 45/75 | Train Loss: 1.2796 | Val Loss: 1.4056 | Train Acc: 0.523 | Val Acc: 0.408 \n",
      "Epoch 50/75 | Train Loss: 1.2934 | Val Loss: 1.4268 | Train Acc: 0.471 | Val Acc: 0.353 \n",
      "Epoch 55/75 | Train Loss: 1.2865 | Val Loss: 1.4216 | Train Acc: 0.493 | Val Acc: 0.353 \n",
      "Epoch 60/75 | Train Loss: 1.2874 | Val Loss: 1.4122 | Train Acc: 0.506 | Val Acc: 0.353 \n",
      "Epoch 65/75 | Train Loss: 1.2903 | Val Loss: 1.4122 | Train Acc: 0.489 | Val Acc: 0.353 \n",
      "Epoch 70/75 | Train Loss: 1.2897 | Val Loss: 1.4122 | Train Acc: 0.511 | Val Acc: 0.353 \n",
      "Epoch 75/75 | Train Loss: 1.2905 | Val Loss: 1.4128 | Train Acc: 0.505 | Val Acc: 0.353 \n",
      "Subject 11 FINAL BEST ACC: 41.91%\n",
      "\n",
      "Processing SUBJECT: 12\n",
      "Epoch 1/75 | Train Loss: 1.8946 | Val Loss: 1.3845 | Train Acc: 0.255 | Val Acc: 0.311 (*) BEST\n",
      "Epoch 2/75 | Train Loss: 1.3921 | Val Loss: 1.3631 | Train Acc: 0.267 | Val Acc: 0.331 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.3805 | Val Loss: 1.4163 | Train Acc: 0.286 | Val Acc: 0.173 \n",
      "Epoch 10/75 | Train Loss: 1.3775 | Val Loss: 1.3931 | Train Acc: 0.299 | Val Acc: 0.259 \n",
      "Epoch 13/75 | Train Loss: 1.3768 | Val Loss: 1.3616 | Train Acc: 0.284 | Val Acc: 0.382 (*) BEST\n",
      "Epoch 15/75 | Train Loss: 1.3682 | Val Loss: 1.3847 | Train Acc: 0.361 | Val Acc: 0.316 \n",
      "Epoch 20/75 | Train Loss: 1.3568 | Val Loss: 1.4193 | Train Acc: 0.371 | Val Acc: 0.035 \n",
      "Epoch 25/75 | Train Loss: 1.3442 | Val Loss: 1.4015 | Train Acc: 0.395 | Val Acc: 0.119 \n",
      "Epoch 30/75 | Train Loss: 1.3339 | Val Loss: 1.3871 | Train Acc: 0.430 | Val Acc: 0.445 (*) BEST\n",
      "Epoch 35/75 | Train Loss: 1.3128 | Val Loss: 1.3936 | Train Acc: 0.456 | Val Acc: 0.197 \n",
      "Epoch 40/75 | Train Loss: 1.3125 | Val Loss: 1.4115 | Train Acc: 0.433 | Val Acc: 0.131 \n",
      "Epoch 45/75 | Train Loss: 1.2988 | Val Loss: 1.3811 | Train Acc: 0.490 | Val Acc: 0.226 \n",
      "Epoch 50/75 | Train Loss: 1.2864 | Val Loss: 1.3725 | Train Acc: 0.501 | Val Acc: 0.248 \n",
      "Epoch 55/75 | Train Loss: 1.2859 | Val Loss: 1.3771 | Train Acc: 0.510 | Val Acc: 0.248 \n",
      "Epoch 60/75 | Train Loss: 1.2869 | Val Loss: 1.3802 | Train Acc: 0.502 | Val Acc: 0.248 \n",
      "Epoch 65/75 | Train Loss: 1.2838 | Val Loss: 1.3706 | Train Acc: 0.510 | Val Acc: 0.248 \n",
      "Epoch 70/75 | Train Loss: 1.2865 | Val Loss: 1.3751 | Train Acc: 0.512 | Val Acc: 0.248 \n",
      "Epoch 75/75 | Train Loss: 1.2917 | Val Loss: 1.3788 | Train Acc: 0.506 | Val Acc: 0.248 \n",
      "Subject 12 FINAL BEST ACC: 44.49%\n",
      "\n",
      "Processing SUBJECT: 13\n",
      "Epoch 1/75 | Train Loss: 1.7292 | Val Loss: 1.3716 | Train Acc: 0.264 | Val Acc: 0.320 (*) BEST\n",
      "Epoch 2/75 | Train Loss: 1.3876 | Val Loss: 1.3397 | Train Acc: 0.277 | Val Acc: 0.382 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.3707 | Val Loss: 1.4027 | Train Acc: 0.295 | Val Acc: 0.173 \n",
      "Epoch 7/75 | Train Loss: 1.3664 | Val Loss: 1.3783 | Train Acc: 0.338 | Val Acc: 0.384 (*) BEST\n",
      "Epoch 10/75 | Train Loss: 1.3368 | Val Loss: 1.3562 | Train Acc: 0.404 | Val Acc: 0.224 \n",
      "Epoch 13/75 | Train Loss: 1.3207 | Val Loss: 1.3857 | Train Acc: 0.416 | Val Acc: 0.483 (*) BEST\n",
      "Epoch 15/75 | Train Loss: 1.2974 | Val Loss: 1.3652 | Train Acc: 0.443 | Val Acc: 0.259 \n",
      "Epoch 20/75 | Train Loss: 1.2547 | Val Loss: 1.4080 | Train Acc: 0.510 | Val Acc: 0.173 \n",
      "Epoch 25/75 | Train Loss: 1.2248 | Val Loss: 1.4225 | Train Acc: 0.511 | Val Acc: 0.340 \n",
      "Epoch 30/75 | Train Loss: 1.2130 | Val Loss: 1.3973 | Train Acc: 0.589 | Val Acc: 0.333 \n",
      "Epoch 35/75 | Train Loss: 1.1961 | Val Loss: 1.4165 | Train Acc: 0.579 | Val Acc: 0.333 \n",
      "Epoch 40/75 | Train Loss: 1.1906 | Val Loss: 1.3925 | Train Acc: 0.568 | Val Acc: 0.290 \n",
      "Epoch 45/75 | Train Loss: 1.2012 | Val Loss: 1.3908 | Train Acc: 0.569 | Val Acc: 0.270 \n",
      "Epoch 50/75 | Train Loss: 1.1918 | Val Loss: 1.3951 | Train Acc: 0.586 | Val Acc: 0.283 \n",
      "Epoch 55/75 | Train Loss: 1.1814 | Val Loss: 1.3996 | Train Acc: 0.601 | Val Acc: 0.333 \n",
      "Epoch 60/75 | Train Loss: 1.1785 | Val Loss: 1.4015 | Train Acc: 0.599 | Val Acc: 0.333 \n",
      "Epoch 65/75 | Train Loss: 1.1856 | Val Loss: 1.3965 | Train Acc: 0.598 | Val Acc: 0.316 \n",
      "Epoch 70/75 | Train Loss: 1.1899 | Val Loss: 1.3960 | Train Acc: 0.597 | Val Acc: 0.320 \n",
      "Epoch 75/75 | Train Loss: 1.1863 | Val Loss: 1.3982 | Train Acc: 0.587 | Val Acc: 0.333 \n",
      "Subject 13 FINAL BEST ACC: 48.35%\n",
      "\n",
      "Processing SUBJECT: 14\n",
      "Epoch 1/75 | Train Loss: 1.6865 | Val Loss: 1.4278 | Train Acc: 0.262 | Val Acc: 0.173 (*) BEST\n",
      "Epoch 2/75 | Train Loss: 1.3871 | Val Loss: 1.3852 | Train Acc: 0.270 | Val Acc: 0.331 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.3552 | Val Loss: 1.4066 | Train Acc: 0.350 | Val Acc: 0.180 \n",
      "Epoch 9/75 | Train Loss: 1.3203 | Val Loss: 1.3736 | Train Acc: 0.413 | Val Acc: 0.382 (*) BEST\n",
      "Epoch 10/75 | Train Loss: 1.3170 | Val Loss: 1.3396 | Train Acc: 0.403 | Val Acc: 0.382 \n",
      "Epoch 15/75 | Train Loss: 1.2455 | Val Loss: 1.4633 | Train Acc: 0.523 | Val Acc: 0.221 \n",
      "Epoch 20/75 | Train Loss: 1.1714 | Val Loss: 1.3622 | Train Acc: 0.599 | Val Acc: 0.401 (*) BEST\n",
      "Epoch 21/75 | Train Loss: 1.1453 | Val Loss: 1.3150 | Train Acc: 0.629 | Val Acc: 0.428 (*) BEST\n",
      "Epoch 25/75 | Train Loss: 1.0839 | Val Loss: 1.3294 | Train Acc: 0.630 | Val Acc: 0.285 \n",
      "Epoch 30/75 | Train Loss: 0.9921 | Val Loss: 1.2591 | Train Acc: 0.695 | Val Acc: 0.311 \n",
      "Epoch 33/75 | Train Loss: 0.9286 | Val Loss: 1.2754 | Train Acc: 0.746 | Val Acc: 0.438 (*) BEST\n",
      "Epoch 35/75 | Train Loss: 0.9236 | Val Loss: 1.3060 | Train Acc: 0.729 | Val Acc: 0.347 \n",
      "Epoch 37/75 | Train Loss: 0.8813 | Val Loss: 1.2711 | Train Acc: 0.772 | Val Acc: 0.487 (*) BEST\n",
      "Epoch 39/75 | Train Loss: 0.8749 | Val Loss: 1.3136 | Train Acc: 0.783 | Val Acc: 0.491 (*) BEST\n",
      "Epoch 40/75 | Train Loss: 0.8752 | Val Loss: 1.2919 | Train Acc: 0.778 | Val Acc: 0.596 (*) BEST\n",
      "Epoch 45/75 | Train Loss: 0.8320 | Val Loss: 1.2662 | Train Acc: 0.820 | Val Acc: 0.452 \n",
      "Epoch 50/75 | Train Loss: 0.8044 | Val Loss: 1.2740 | Train Acc: 0.828 | Val Acc: 0.438 \n",
      "Epoch 55/75 | Train Loss: 0.7990 | Val Loss: 1.2805 | Train Acc: 0.839 | Val Acc: 0.550 \n",
      "Epoch 60/75 | Train Loss: 0.7881 | Val Loss: 1.2502 | Train Acc: 0.843 | Val Acc: 0.450 \n",
      "Epoch 65/75 | Train Loss: 0.7790 | Val Loss: 1.2662 | Train Acc: 0.852 | Val Acc: 0.482 \n",
      "Epoch 70/75 | Train Loss: 0.7725 | Val Loss: 1.2528 | Train Acc: 0.855 | Val Acc: 0.482 \n",
      "Epoch 75/75 | Train Loss: 0.7761 | Val Loss: 1.2443 | Train Acc: 0.859 | Val Acc: 0.483 \n",
      "Subject 14 FINAL BEST ACC: 59.56%\n",
      "\n",
      "Processing SUBJECT: 15\n",
      "Epoch 1/75 | Train Loss: 1.8691 | Val Loss: 1.3292 | Train Acc: 0.264 | Val Acc: 0.320 (*) BEST\n",
      "Epoch 2/75 | Train Loss: 1.3771 | Val Loss: 1.3634 | Train Acc: 0.290 | Val Acc: 0.382 (*) BEST\n",
      "Epoch 3/75 | Train Loss: 1.3662 | Val Loss: 1.3449 | Train Acc: 0.328 | Val Acc: 0.493 (*) BEST\n",
      "Epoch 5/75 | Train Loss: 1.3303 | Val Loss: 1.2963 | Train Acc: 0.382 | Val Acc: 0.379 \n",
      "Epoch 7/75 | Train Loss: 1.2844 | Val Loss: 1.2040 | Train Acc: 0.451 | Val Acc: 0.743 (*) BEST\n",
      "Epoch 9/75 | Train Loss: 1.1817 | Val Loss: 1.0945 | Train Acc: 0.563 | Val Acc: 0.789 (*) BEST\n",
      "Epoch 10/75 | Train Loss: 1.1154 | Val Loss: 0.9436 | Train Acc: 0.618 | Val Acc: 0.702 \n",
      "Epoch 14/75 | Train Loss: 0.8794 | Val Loss: 0.7846 | Train Acc: 0.789 | Val Acc: 0.814 (*) BEST\n",
      "Epoch 15/75 | Train Loss: 0.8260 | Val Loss: 0.7752 | Train Acc: 0.813 | Val Acc: 0.846 (*) BEST\n",
      "Epoch 17/75 | Train Loss: 0.7917 | Val Loss: 0.8635 | Train Acc: 0.807 | Val Acc: 0.871 (*) BEST\n",
      "Epoch 19/75 | Train Loss: 0.6792 | Val Loss: 0.7309 | Train Acc: 0.870 | Val Acc: 0.906 (*) BEST\n",
      "Epoch 20/75 | Train Loss: 0.6399 | Val Loss: 0.6391 | Train Acc: 0.888 | Val Acc: 0.921 (*) BEST\n",
      "Epoch 21/75 | Train Loss: 0.6266 | Val Loss: 0.6632 | Train Acc: 0.894 | Val Acc: 0.949 (*) BEST\n",
      "Epoch 25/75 | Train Loss: 0.5363 | Val Loss: 0.7133 | Train Acc: 0.930 | Val Acc: 0.851 \n",
      "Epoch 30/75 | Train Loss: 0.5121 | Val Loss: 0.5821 | Train Acc: 0.940 | Val Acc: 0.811 \n",
      "Epoch 35/75 | Train Loss: 0.4573 | Val Loss: 0.5590 | Train Acc: 0.989 | Val Acc: 0.886 \n",
      "Epoch 40/75 | Train Loss: 0.4431 | Val Loss: 0.5666 | Train Acc: 0.993 | Val Acc: 0.860 \n",
      "Epoch 45/75 | Train Loss: 0.4345 | Val Loss: 0.5609 | Train Acc: 0.995 | Val Acc: 0.886 \n",
      "Epoch 50/75 | Train Loss: 0.4264 | Val Loss: 0.5641 | Train Acc: 0.997 | Val Acc: 0.871 \n",
      "Epoch 55/75 | Train Loss: 0.4193 | Val Loss: 0.5629 | Train Acc: 0.996 | Val Acc: 0.886 \n",
      "Epoch 60/75 | Train Loss: 0.4187 | Val Loss: 0.5742 | Train Acc: 0.997 | Val Acc: 0.886 \n",
      "Epoch 65/75 | Train Loss: 0.4162 | Val Loss: 0.5675 | Train Acc: 0.997 | Val Acc: 0.886 \n",
      "Epoch 70/75 | Train Loss: 0.4135 | Val Loss: 0.5741 | Train Acc: 0.999 | Val Acc: 0.884 \n",
      "Epoch 75/75 | Train Loss: 0.4148 | Val Loss: 0.5659 | Train Acc: 0.997 | Val Acc: 0.886 \n",
      "Subject 15 FINAL BEST ACC: 94.85%\n",
      "\n",
      "==================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "==================================================\n",
      "Subjects: 15 | Average Accuracy: 63.25% | Std: 18.43\n",
      "Accuracies List: [46.88, 77.21, 49.45, 76.29, 41.18, 78.49, 84.19, 43.01, 82.54, 80.33, 41.91, 44.49, 48.35, 59.56, 94.85]\n"
     ]
    }
   ],
   "source": [
    "all_subject_accuracies = []\n",
    "\n",
    "for subject_id in subjects:\n",
    "    print(f\"\\nProcessing SUBJECT: {subject_id}\")\n",
    "    \n",
    "    # --- Data Split by Trials ---\n",
    "    sub_df = dataset.info[dataset.info['subject_id'] == subject_id]\n",
    "    all_trials = list(range(1, 25))\n",
    "    random.seed(42)\n",
    "    test_trials = random.sample(all_trials, 5)\n",
    "    train_trials = [t for t in all_trials if t not in test_trials]\n",
    "\n",
    "    train_indices = sub_df[sub_df['trial_id'].isin(train_trials)].index.tolist()\n",
    "    test_indices = sub_df[sub_df['trial_id'].isin(test_trials)].index.tolist()\n",
    "\n",
    "    train_set = Subset(dataset, train_indices)\n",
    "    test_set = Subset(dataset, test_indices)\n",
    "\n",
    "    # --- Sampler for Class Imbalance ---\n",
    "    y_train_indices = train_set.indices\n",
    "    raw_labels = dataset.info.iloc[y_train_indices]['emotion'].values\n",
    "    class_counts = np.bincount(raw_labels)\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = [class_weights[y] for y in raw_labels]\n",
    "\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=64, sampler=sampler, num_workers=0)\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "    # --- Model ---\n",
    "    model = CCNN(\n",
    "        num_classes=4,\n",
    "        in_channels=5,        \n",
    "        grid_size=(9, 9),     \n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    # --- Criterion & Optimizer ---\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    epochs = 75\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss_sum = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_sum += loss.item()\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        avg_train_loss = train_loss_sum / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, test_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        is_best = False\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            is_best = True\n",
    "            \n",
    "            save_path = os.path.join(save_dir, f\"best_model_subject_{subject_id}.pth\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        \n",
    "\n",
    "        if is_best or (epoch + 1) % 5 == 0:\n",
    "            status = \"(*) BEST\" if is_best else \"\"\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Train Acc: {train_acc:.3f} | Val Acc: {val_acc:.3f} {status}\")\n",
    "\n",
    "    all_subject_accuracies.append(best_acc)\n",
    "    print(f\"Subject {subject_id} FINAL BEST ACC: {best_acc*100:.2f}%\")\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "final_avg_acc = np.mean(all_subject_accuracies)\n",
    "final_std_acc = np.std(all_subject_accuracies)\n",
    "print(f\"Subjects: {len(subjects)} | Average Accuracy: {final_avg_acc*100:.2f}% | Std: {final_std_acc*100:.2f}\")\n",
    "print(f\"Accuracies List: {[round(x*100,2) for x in all_subject_accuracies]}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 218098,
     "sourceId": 472319,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2059.286008,
   "end_time": "2025-12-11T14:43:31.368706",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-11T14:09:12.082698",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
